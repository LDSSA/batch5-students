{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05e844b835aa5580",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SLU13: Bias-Variance trade-off & Model Selection -- Exercises\n",
    "---\n",
    "\n",
    "*Exercises are graded unless otherwise indicated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a717b54f5012a755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-37bf85a5bd1cad30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Data\n",
    "During the learning notebook you've predicted if a beer would be an IPA or not based on a couple of features. For this exercise notebook we'll take it a steep further and try to predict prices of wine based on their quality. \n",
    "\n",
    "_Real classy, huh?_\n",
    "\n",
    "<img alt=\"wine\" src=\"media/wine.png\" width=\"400\">\n",
    "\n",
    "Start by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7f9a1aabe452b5eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function is designed to be used in all the exercises and make sure the dataset used is stable\n",
    "\n",
    "def load_dataset():\n",
    "    # Loads wine prices dataset\n",
    "    df = pd.read_csv(\"data/wine_prices.csv\")\n",
    "    \n",
    "    # Sorts the data to make sure plots will appear nicely later in the notebook later on ;) \n",
    "    df_wine = df.set_index('Rating', drop=False).sort_index()\n",
    "\n",
    "    return df_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b16f0324653985e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_wine = load_dataset()\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-843380c1fe8cc4f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_wine.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7f11413fcd7c42b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To simplify, we'll ignore other features and only use the ratings of the wines to predict its prices. So we'll set our (X, y) pair and plot the data points we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-77851ebe926013cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = df_wine[['Rating']]\n",
    "y = df_wine['Price']\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X, y, c='orange', s=5)\n",
    "plt.title('Wine price vs rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Price')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f08f6d01a41b642",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Bias-variance trade-off\n",
    "\n",
    "So let's say we want to create a model to predict the relationship above so we decide to try a couple of the models you've learned about to do it.\n",
    "\n",
    "Start by implementing a function to fit and return the mean squared error of a simple `LinearRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-528009c6cae73531",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fit_lr_estimator(X, y):\n",
    "    \"\"\" \n",
    "        Fits `LinearRegression` and predict mean squared error of \n",
    "        predictions for the provided data.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): the input DataFrame\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: estimator, error\n",
    "        estimator (LinearRegression): fitted estimator\n",
    "        error (float): mean squared error in provided data\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # estimator = ...\n",
    "    # error = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return estimator, error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba799076ff5a31d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Verify that your code passes the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6cf1d205761bc989",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "X = df_wine[['Rating']]\n",
    "y = df_wine['Price']\n",
    "\n",
    "lr, lr_error = fit_lr_estimator(X, y)\n",
    "\n",
    "predictions = lr.predict(X)\n",
    "\n",
    "np.testing.assert_almost_equal(predictions[10], -23.3191, 2)\n",
    "np.testing.assert_almost_equal(predictions[123], -7.2025, 2)\n",
    "np.testing.assert_almost_equal(lr_error, 750.890, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5fa6d81872f2011d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now plot the data to see how well our model estimates the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3417ca3bea8d842c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X, y, c='orange', s=5, label=\"Original\")\n",
    "plt.plot(X, predictions, label=\"Linear regression\")\n",
    "plt.legend()\n",
    "plt.title('Linear regression fit')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Price')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4d757b7a17eca3a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q1.1) What can you say about this model:**\n",
    "    \n",
    "- A) It has a high bias and low variance, as it is overly flexible and overreact to the specifics of the training data\n",
    "- B) It has a low bias and high variance, as it makes over-simplistic assumptions about the distribution\n",
    "- C) It has a high bias and low variance, as it makes over-simplistic assumptions about the distribution\n",
    "- D) None of the above\n",
    "\n",
    "\n",
    "Enter your answer below wrapped by quotes, for example:\n",
    "\n",
    "```\n",
    "answer_q11 = \"A\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbb947bf947b3288",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_q11 = 'A' or 'B' or 'C' or 'D'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8a908ac3ddb073ec",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer_q11.encode()) == b'Qw=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now decide that you want to try a slightly more complex approach, and by looking at the data you wonder if adding some polynomial features would help.\n",
    "\n",
    "You create a function to provide polynomial features for your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-037b7a9c18d4ec0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_features(X, degree=2):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    return X_poly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ef3389338127695",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You apply it with some different degrees and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_5 =  polynomial_features(X, degree=5)\n",
    "X_poly_20 = polynomial_features(X, degree=20)\n",
    "\n",
    "lr_poly_5, lr_error_poly_5 = fit_lr_estimator(X_poly_5, y)\n",
    "predictions_poly_5 = lr_poly_5.predict(X_poly_5)\n",
    "\n",
    "lr_poly_20, lr_error_poly_20 = fit_lr_estimator(X_poly_20, y)\n",
    "predictions_poly_20 = lr_poly_20.predict(X_poly_20)\n",
    "\n",
    "\n",
    "X = df_wine[['Rating']]\n",
    "y = df_wine[['Price']]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X, y, c='orange', s=5, label=\"Original\")\n",
    "plt.plot(X, predictions_poly_5, c='blue', label=\"Polynomial 5\")\n",
    "plt.plot(X, predictions_poly_20, c='green', label=\"Polynomial 20\")\n",
    "plt.legend()\n",
    "plt.title('Polynomial regressions fit')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Price')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-234097897a558f4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q1.2) What can you say about the polynomial with degree 20:**\n",
    "    \n",
    "- A) It has a low bias and high variance, as it makes over-simplistic assumptions about the distribution\n",
    "- B) It has a low bias and high variance, as it is overly flexible and overreact to the specifics of the training data\n",
    "- C) It's a perfect fit for the data\n",
    "- D) None of the above\n",
    "\n",
    "Enter your answer below wrapped by quotes, for example:\n",
    "\n",
    "```\n",
    "answer_q12 = \"A\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aad20e3118628f9d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_q12 = 'A' or 'B' or 'C' or 'D'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-41576dd1f26748eb",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer_q12.encode()) == b'Qg=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59c63fa6c82ea823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, you decide to plot all the errors to decide which model is best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = {\n",
    "    \"linear\": lr_error,\n",
    "    \"polynomial (degree=5)\": lr_error_poly_5,\n",
    "    \"polynomial (degree=20)\": lr_error_poly_20,\n",
    "\n",
    "}\n",
    "\n",
    "pd.Series(training_error).plot(figsize=(7, 5), kind='bar', rot=25)\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Training error per regressor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-695abdb560c14a09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q1.3) If you were to choose based on the training error, which one would you choose?**\n",
    "    \n",
    "- A) Linear\n",
    "- B) Polynomial with degree 5\n",
    "- C) Polynomial with degree 20\n",
    "\n",
    "Enter your answer below wrapped by quotes, for example:\n",
    "\n",
    "```\n",
    "answer_q13 = \"A\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abd0bcfc699f47f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_q13 = 'A' or 'B' or 'C' \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-747939f0516b8196",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer_q13.encode()) == b'Qw=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7aa3117bb40c127b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Train/validation/test split\n",
    "\n",
    "You now decide to try out a few of the methods you've learned in this unit. \n",
    "\n",
    "You start with the holdout method. Create a function that splits your data into train and test set. Assume the test set is **35%** of the full dataset. Use a **random state** of 42\n",
    "\n",
    "Implement that function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd7765c4a31f2393",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def holdout_method(X, y):\n",
    "    \"\"\" \n",
    "        Implement the holdout method: a train test split with proportion 65-35, \n",
    "        this is, where the test size should be 35% of the size of the dataset\n",
    "        \n",
    "        **For reproducibility: Use random state of 42 always**\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): the input DataFrame X\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "        X_train (pd.DataFrame): the input DataFrame X of the training sample \n",
    "        X_test (pd.DataFrame): the input DataFrame X of the test sample\n",
    "        y_train (pd.Series): the target labels of the training sample\n",
    "        y_test (pd.Series): the target labels of the test sample\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec73e4fa95dfb3dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your function below to ensure it returns the desired outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f5ebca320c304467",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "X = df_wine[['Rating']]\n",
    "y = df_wine['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = holdout_method(X, y)\n",
    "\n",
    "np.testing.assert_almost_equal(X_train.Rating.values[10], 3.8, 2)\n",
    "np.testing.assert_almost_equal(X_train.Rating.values[42], 3.9, 2)\n",
    "np.testing.assert_almost_equal(X_train.Rating.values[1402], 4.5, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(X_test.Rating.values[2], 3.4, 2)\n",
    "np.testing.assert_almost_equal(X_test.Rating.values[33], 4.3, 2)\n",
    "np.testing.assert_almost_equal(X_test.Rating.values[932], 3.7, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(y_train.values[213], 8.99, 2)\n",
    "np.testing.assert_almost_equal(y_train.values[677], 9.99, 2)\n",
    "np.testing.assert_almost_equal(y_train.values[2000], 11.35, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(y_test.values[114], 9.45, 2)\n",
    "np.testing.assert_almost_equal(y_test.values[277], 7.90, 2)\n",
    "np.testing.assert_almost_equal(y_test.values[1000], 6.90, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ada034ee09782c96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can now use your function to split your dataset and train only on a sample of it. Run the cell below to do so and plot the resulting predictions for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = holdout_method(X, y)\n",
    "\n",
    "# Compute polynomial features with degree 5\n",
    "X_train_poly_5 =  polynomial_features(X_train, degree=5)\n",
    "X_test_poly_5 =  polynomial_features(X_test, degree=5)\n",
    "\n",
    "# Compute polynomial features with degree 50\n",
    "X_train_poly_20 = polynomial_features(X_train, degree=20)\n",
    "X_test_poly_20 = polynomial_features(X_test, degree=20)\n",
    "\n",
    "# Fit all estimators\n",
    "lr, _ = fit_lr_estimator(X_train, y_train)\n",
    "lr_poly_5, lr_error_poly_5 = fit_lr_estimator(X_train_poly_5, y_train)\n",
    "lr_poly_20, lr_error_poly_20 = fit_lr_estimator(X_train_poly_20, y_train)\n",
    "\n",
    "predictions_lr = lr.predict(X_train)\n",
    "predictions_poly_5 = lr_poly_5.predict(X_train_poly_5)\n",
    "predictions_poly_20 = lr_poly_20.predict(X_train_poly_20)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train, y_train, c='orange', s=5, label=\"Original\")\n",
    "plt.plot(X_train, predictions_lr, c='blue', label=\"Linear\")\n",
    "\n",
    "X_train_plot, predictions_poly_5_plot = zip(*sorted(zip(X_train.values, predictions_poly_5)))\n",
    "plt.plot(X_train_plot, predictions_poly_5_plot, c='brown', label=\"Polynomial 5\")\n",
    "\n",
    "X_train_plot, predictions_poly_20_plot = zip(*sorted(zip(X_train.values, predictions_poly_20)))\n",
    "plt.plot(X_train_plot, predictions_poly_20_plot, c='green', label=\"Polynomial 20\")\n",
    "plt.legend()\n",
    "plt.title('Regressions fit on training data')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Price')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec2e889f8a24a330",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Our data changed but the effects of the different estimators are still visible as before. Answer the following question:\n",
    "    \n",
    "**Q1.4) What would you say about the models above?**\n",
    "    \n",
    "- A) `Linear` model is underfitting and `Polynomial 20` overfitting\n",
    "- B) Both `Linear` model and `Polynomial 5` are overfitting\n",
    "- C) `Linear` model is overfitting and `Polynomial 20` underfitting\n",
    "- D) All models are underfitting\n",
    "\n",
    "Enter your answer below wrapped by quotes, for example:\n",
    "\n",
    "```\n",
    "answer_q14 = \"A\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13fe8c63e6df7a62",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_q14 = 'A' or 'B' or 'C' \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9fd510d552f58e68",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer_q14.encode()) == b'QQ=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93021be6f5d7ad55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, we want to see the different In-sample and Out-of-sample errors. Start by implementing a function to return both the predictions of an estimator and the associated regression error, in this particular case, the mean squared error.\n",
    "\n",
    "Implement the function below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8572714f498d1880",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def regression_error(estimator, X, y):\n",
    "    \"\"\" \n",
    "        Implement a method that takes a regressor `estimator`, predicts values for \n",
    "        the provided features `X` and returns both the predictions and\n",
    "        mean squared error\n",
    "    \n",
    "    Args:\n",
    "        estimator (sklearn.base.BaseEstimator): estimator with predict method\n",
    "        X (pd.DataFrame): the input DataFrame X\n",
    "        y (pd.Series): the target true labels\n",
    "\n",
    "    Returns: predictions, error\n",
    "        predictions (pd.Series): the preidcted labels\n",
    "        error (float): mean squared error in provided data\n",
    "    \"\"\"\n",
    "    \n",
    "    # predictions = ...\n",
    "    # error = ...\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return predictions, error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50b17ae89d592014",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it in the assertions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bb7c4a25a5c11b92",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "X = df_wine[['Rating']]\n",
    "y = df_wine['Price']\n",
    "\n",
    "# estimator to be used for these tests only\n",
    "estimator = DecisionTreeRegressor()\n",
    "estimator.fit(X, y)\n",
    "\n",
    "preditions, error = regression_error(estimator, X, y)\n",
    "\n",
    "np.testing.assert_almost_equal(preditions[10], 7.2159, 2)\n",
    "np.testing.assert_almost_equal(preditions[42], 9.0032, 2)\n",
    "np.testing.assert_almost_equal(preditions[1402], 12.4064, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(error, 456.3274, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4306982426c4de51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can now use this function to get our training (In-sample) and test (Out-of-sample) errors. Run the cell below to compute them and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c57224fee9c479b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Predict for train and test samples and plot errors\n",
    "lr_train_preds, lr_train_error = regression_error(lr, X_train, y_train)\n",
    "lr_test_preds, lr_test_error = regression_error(lr, X_test, y_test)\n",
    "\n",
    "lr_poly_5_train_preds, lr_poly_5_train_error = regression_error(lr_poly_5, X_train_poly_5, y_train)\n",
    "lr_poly_5_test_preds, lr_poly_5_test_error = regression_error(lr_poly_5, X_test_poly_5, y_test)\n",
    "\n",
    "lr_poly_20_train_preds, lr_poly_20_train_error = regression_error(lr_poly_20, X_train_poly_20, y_train)\n",
    "lr_poly_20_test_preds, lr_poly_20_test_error = regression_error(lr_poly_20, X_test_poly_20, y_test)\n",
    "\n",
    "training_error = pd.Series({\n",
    "    \"linear\": lr_train_error,\n",
    "    \"polynomial (degree=5)\": lr_poly_5_train_error,\n",
    "    \"polynomial (degree=20)\": lr_poly_20_train_error,\n",
    "\n",
    "})\n",
    "\n",
    "testing_error = pd.Series({\n",
    "    \"linear\": lr_test_error,\n",
    "    \"polynomial (degree=5)\": lr_poly_5_test_error,\n",
    "    \"polynomial (degree=20)\": lr_poly_20_test_error,\n",
    "\n",
    "})\n",
    "\n",
    "index = ['linear', 'polynomial (degree=5)', 'polynomial (degree=20)']\n",
    "df = pd.DataFrame({'Training error': training_error, 'Test error': testing_error}, index=index)\n",
    "\n",
    "plt.figure(figsize=(7, 5));\n",
    "df.plot.bar(rot=25);\n",
    "\n",
    "plt.ylabel('Training Error');\n",
    "plt.title('Training error per regressor');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-227b665300e5ab1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.1) If you were to choose now based on the test error, which model would you choose?**\n",
    "    \n",
    "- A) Linear\n",
    "- B) Polynomial with degree 5\n",
    "- C) Polynomial with degree 20\n",
    "\n",
    "Enter your answer below wrapped by quotes, for example:\n",
    "\n",
    "```\n",
    "answer_q21 = \"A\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ebca070b91c70ebb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_q21 = 'A' or 'B' or 'C' \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9f655f6b8f5c3cbf",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer_q21.encode()) == b'Qg=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8181a4722d0398dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Great! We've seen how having a held-out test set can lead us into better models than just blindly training on everything. Take it one step further and implement the train-test-validation split: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15cf67bd03cbec3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_test_validation(X, y):\n",
    "    \"\"\" \n",
    "        Implement a train-validation-test split with proportion 50-25-25, this is, \n",
    "        where the validation set and the test set should each be 25% of the size of \n",
    "        the dataset\n",
    "        \n",
    "        **For reproducibility: Use random state of 42 always and sample the validation set first**\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): the input DataFrame X\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "        X_train (pd.DataFrame): the input DataFrame X of the training sample \n",
    "        X_val (pd.DataFrame): the input DataFrame X of the validation sample \n",
    "        X_test (pd.DataFrame): the input DataFrame X of the test sample\n",
    "        y_train (pd.Series): the target labels of the training sample\n",
    "        y_val (pd.Series): the target labels of the validation sample\n",
    "        y_test (pd.Series): the target labels of the test sample\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f09bab74460363b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your function below to ensure it returns the desired outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ac7f084892997aa0",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "X = df_wine[['Rating']]\n",
    "y = df_wine['Price']\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_validation(X, y)\n",
    "\n",
    "np.testing.assert_almost_equal(X_train.Rating.values[10], 3.8, 2)\n",
    "np.testing.assert_almost_equal(X_train.Rating.values[42], 3.4, 2)\n",
    "np.testing.assert_almost_equal(X_train.Rating.values[1402], 3.5, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(X_val.Rating.values[14], 4.1, 2)\n",
    "np.testing.assert_almost_equal(X_val.Rating.values[59], 3.8, 2)\n",
    "np.testing.assert_almost_equal(X_val.Rating.values[422], 3.9, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(X_test.Rating.values[2], 4.1, 2)\n",
    "np.testing.assert_almost_equal(X_test.Rating.values[33], 3.8, 2)\n",
    "np.testing.assert_almost_equal(X_test.Rating.values[632], 4.0, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(y_train.values[213], 263.9, 2)\n",
    "np.testing.assert_almost_equal(y_train.values[677], 11.94, 2)\n",
    "np.testing.assert_almost_equal(y_train.values[2000], 29.45, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(y_val.values[123], 9.95, 2)\n",
    "np.testing.assert_almost_equal(y_val.values[518], 14.90, 2)\n",
    "np.testing.assert_almost_equal(y_val.values[663], 8.90, 2)\n",
    "\n",
    "np.testing.assert_almost_equal(y_test.values[114], 23.9, 2)\n",
    "np.testing.assert_almost_equal(y_test.values[277], 8.45, 2)\n",
    "np.testing.assert_almost_equal(y_test.values[700], 7.29, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c934df8b36fe4985",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can now see how you could use this validation set, say, to optimize the polynomial degree used, and then check the final performance on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_validation(X, y)\n",
    "\n",
    "polynomial_train_error = {}\n",
    "polynomial_val_error = {}\n",
    "polynomial_test_error = {}\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train, y_train, c='orange', s=5, label=\"Original\")\n",
    "\n",
    "for degree in [2, 3, 4, 5, 7, 9, 15, 20]:\n",
    "    polynomial_label = \"Polynomial {}\".format(degree)\n",
    "    \n",
    "    X_train_poly =  polynomial_features(X_train, degree=degree)\n",
    "    X_val_poly =  polynomial_features(X_val, degree=degree)\n",
    "    X_test_poly =  polynomial_features(X_test, degree=degree)\n",
    "\n",
    "    lr_poly, lr_error_poly = fit_lr_estimator(X_train_poly, y_train)\n",
    "\n",
    "    lr_poly_train_preds, lr_poly_train_error = regression_error(lr_poly, X_train_poly, y_train)\n",
    "    lr_poly_val_preds, lr_poly_val_error = regression_error(lr_poly, X_val_poly, y_val)\n",
    "    lr_poly_test_preds, lr_poly_test_error = regression_error(lr_poly, X_test_poly, y_test)\n",
    "\n",
    "    polynomial_train_error.update({polynomial_label: lr_poly_train_error})\n",
    "    polynomial_val_error.update({polynomial_label: lr_poly_val_error})\n",
    "    polynomial_test_error.update({polynomial_label: lr_poly_test_error})\n",
    "\n",
    "    X_train_plot, predictions_poly_plot = zip(*sorted(zip(X_train.values, lr_poly_train_preds)))\n",
    "    plt.plot(X_train_plot, predictions_poly_plot, label=polynomial_label)\n",
    "\n",
    "    \n",
    "plt.legend()\n",
    "plt.title('Polynomial regressions fit on validation data')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Price')\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-266b36a37ac9b172",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see roughly that some fit better than others but it doesn't automatically tell us which one is best. Let's instead plot the training/validation errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d5e9ff1567c4c0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot errors for all polynomials\n",
    "\n",
    "index = polynomial_val_error.keys()\n",
    "df = pd.DataFrame({'Training error': polynomial_train_error, 'Validation error': polynomial_val_error}, index=index)\n",
    "\n",
    "plt.figure(figsize=(20, 20));\n",
    "df.plot.bar(rot=25);\n",
    "\n",
    "plt.ylim(0, 5000)\n",
    "plt.ylabel('Training Error');\n",
    "plt.title('Training error per regressor');\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14defffbd61063a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Seems like we're getting there. The next step would be to really extract the polynomial degree that yields the lowest error. Let's do that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_errors = sorted(polynomial_val_error.items(), key=lambda x: x[1])\n",
    "min_error = sorted_errors[0][1]\n",
    "min_degree = sorted_errors[0][0]\n",
    "\n",
    "print(\"Minimum validation error at {}: {}\".format(min_degree, min_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-690f5d81ae3a410b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's finally plot the test errors comparing both our optimized polynomial, the 2 previous random values (5 and 20) and the initial linear regression to verify that optimizing using the validation set will reflect finally on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, lr_error = fit_lr_estimator(X_train, y_train)\n",
    "predictions_lr = lr.predict(X_train)\n",
    "\n",
    "lr_train_preds, lr_train_error = regression_error(lr, X_train, y_train)\n",
    "lr_test_preds, lr_val_error = regression_error(lr, X_test, y_test)\n",
    "lr_test_preds, lr_test_error = regression_error(lr, X_test, y_test)\n",
    "\n",
    "train_errors = {\"Linear\": lr_train_error}\n",
    "validation_errors = {\"Linear\": lr_val_error}\n",
    "test_errors = {\"Linear\": lr_test_error}\n",
    "\n",
    "for polynomial in [\n",
    "    \"Polynomial 4\",  # Optimized value\n",
    "    \"Polynomial 5\",  # Previous good value\n",
    "    \"Polynomial 20\", # Previous overfitted value\n",
    "\n",
    "]:\n",
    "    train_errors[polynomial] = polynomial_train_error[polynomial]\n",
    "    validation_errors[polynomial] = polynomial_val_error[polynomial]\n",
    "    test_errors[polynomial] = polynomial_test_error[polynomial]\n",
    "\n",
    "index = test_errors.keys()\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'Training error': train_errors, \n",
    "        'Validation error': validation_errors,\n",
    "        'Test error': test_errors\n",
    "    }, index=index)\n",
    "\n",
    "plt.figure(figsize=(20, 20));\n",
    "df.plot.bar(rot=25);\n",
    "\n",
    "plt.ylim(0, 5000)\n",
    "plt.ylabel('Error');\n",
    "plt.title('Error per regressor');\n",
    "plt.show();\n",
    "\n",
    "sorted_errors = sorted(test_errors.items(), key=lambda x: x[1])\n",
    "min_error = sorted_errors[0][1]\n",
    "min_degree = sorted_errors[0][0]\n",
    "\n",
    "print(\"Minimum test error at {}: {}\".format(min_degree, min_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5dc20c1644ff337a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Congratulations, you've now successfully used the train-validation-test method to optimize a model using a validation set and then compare it fairly on a held-out test set!\n",
    "\n",
    "_This calls for a toast!_\n",
    "\n",
    "<img alt=\"wine_toast\" src=\"media/wine_toast.jpg\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-970ade4434f10e80",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 3: K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79dcb5bbe3ccc495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the final method, we are going to try to take advantage of as much data as possible and use k-fold cross validation to approximate our OSE. \n",
    "\n",
    "Implement a function that runs the `cross_val_score` on a given estimator and returns the average error on 5 folds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bbf1209b1ff0195",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hint: You will need this, feel free to use it directly\n",
    "mse_scorer = make_scorer(mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b0ed9bb14923e9fd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_fold_validation(estimator, X, y):\n",
    "    \"\"\" \n",
    "        Implement a cross validation assessment that returns the mean squared error\n",
    "        score for a 5-fold run on the given estimator\n",
    "            \n",
    "    Args:\n",
    "        estimator (sklearn.base.BaseEstimator): estimator to run cross validation fold on\n",
    "        X (pd.DataFrame): the input DataFrame X\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: mse_cv\n",
    "        mse_cv (float): mean error on the 5-fold run \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return mse_cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e89d8d233b70f834",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "X = df_wine[['Rating']]\n",
    "y = df_wine['Price']\n",
    "\n",
    "lr = LinearRegression()\n",
    "mse_cv_lr = cross_fold_validation(lr, X, y)\n",
    "np.testing.assert_almost_equal(mse_cv_lr, 1065.6411, 2)\n",
    "\n",
    "\n",
    "lr_poly = LinearRegression()\n",
    "X_poly =  polynomial_features(X, degree=4)\n",
    "mse_cv_poly = cross_fold_validation(lr_poly, X_poly, y)\n",
    "\n",
    "np.testing.assert_almost_equal(mse_cv_poly, 1170.41, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-148e37b2669471de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's do the same as before and try to find the best regressor using these cross validation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_cv_error = {}\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "for degree in [2, 3, 4, 5, 7, 9, 15, 20]:\n",
    "    polynomial_label = \"Polynomial {}\".format(degree)\n",
    "    X_poly =  polynomial_features(X, degree=degree)\n",
    "\n",
    "    mse_cv = cross_fold_validation(lr, X_poly, y)\n",
    "\n",
    "    polynomial_cv_error.update({polynomial_label: mse_cv})\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(10, 10));\n",
    "pd.Series(polynomial_cv_error).plot.bar(rot=25);\n",
    "\n",
    "# Capping the chart at 4000 so it's easy to read\n",
    "plt.ylim(0, 5000)\n",
    "plt.ylabel('Error');\n",
    "plt.title('Error per regressor');\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93b4f4ae0d87b72d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sorted_errors = sorted(polynomial_cv_error.items(), key=lambda x: x[1])\n",
    "min_error = sorted_errors[0][1]\n",
    "min_degree = sorted_errors[0][0]\n",
    "\n",
    "print(\"Minimum cross-validation error at {}: {}\".format(min_degree, min_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5d30bcabed8961e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nicely done!\n",
    "\n",
    "Notice that this time we got the polynomial 5 instead of 4, so what's up with that? Well, if you go back and look at the previous method you will see the validation and test errors for this degree were very very close to 4, so it took just a small change in the data used to yield this difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-17878b9c0cf022a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4: Learning curves\n",
    "\n",
    "You made it to the final exercise. In this exercise, you have to figure out if we have enough data or if we should use more. We want you to use the learning curve method you've seen to figure this out.\n",
    "\n",
    "Implement the following: \n",
    "\n",
    "1. Define a numpy array of train_sizes, from 10% of the data to 100%, in increments of 5% (0.1, 0.2, 0.3... etc) \n",
    "\n",
    "\n",
    "2. Get the learning curve data, with the following configuration:\n",
    "    - estimator: estimator to run cross validation fold on \n",
    "    - metric: use mean squared error (you can reuse mse_scorer from before) as your metric \n",
    "    - use the train sizes array you just created\n",
    "    - all features, not normalized \n",
    "    - cv = 5 \n",
    "    - random state = 42 (needed to pass the grader) \n",
    "    - n_jobs = -1 (optional, but faster) \n",
    "\n",
    "As with the learning notebooks you should save the output to `train_sizes_abs`, `train_scores` and `test_scores` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dbf0ece714023360",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_learning_curve(estimator, X, y):\n",
    "    \"\"\" \n",
    "        Implement a learning curve computation for different training sizes,\n",
    "        from 10% of the data to 100%, in increments of 5% (0.1, 0.2, 0.3... etc) \n",
    "        \n",
    "        **For reproducibility: Use random state of 42 always**\n",
    "    \n",
    "    Args:\n",
    "        estimator (sklearn.base.BaseEstimator): estimator to run cross validation fold on\n",
    "        X (pd.DataFrame): the input DataFrame X\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: train_sizes, train_sizes_abs, train_scores, test_scores\n",
    "        train_sizes (float): mean error on the 5-fold run \n",
    "        train_sizes_abs (float): mean error on the 5-fold run \n",
    "        train_scores (float): mean error on the 5-fold run \n",
    "        test_scores (float): mean error on the 5-fold run \n",
    "    \"\"\"\n",
    "    \n",
    "    # train_sizes = ...   (5% increments, starting at 10%)\n",
    "    # train_sizes_abs, train_scores, test_scores\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return train_sizes, train_sizes_abs, train_scores, test_scores\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-61fec38851ea4e8e",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "\n",
    "# Our dataset was sorted, to avoid the slices of data being sorted also we shuffle it\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "X = df[['Rating']]\n",
    "y = df['Price']\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "# We'll use the best fit from the train-val-test method\n",
    "X_poly = PolynomialFeatures(4).fit_transform(X)\n",
    "\n",
    "train_sizes, train_sizes_abs, train_scores, test_scores = get_learning_curve(lr, X_poly, y)\n",
    "\n",
    "assert np.nan not in train_scores \n",
    "assert np.nan not in test_scores \n",
    "\n",
    "np.testing.assert_almost_equal(train_sizes.sum(), 10.45, 2)\n",
    "np.testing.assert_almost_equal(train_sizes.mean(), .55, 2)\n",
    "assert len(train_sizes) == 19\n",
    "\n",
    "np.testing.assert_almost_equal(train_sizes_abs.mean(), 1655.58, 2)\n",
    "np.testing.assert_almost_equal(round(pd.DataFrame(train_scores).mean().median(), 2), 473.71, 2)\n",
    "np.testing.assert_almost_equal(round(pd.DataFrame(test_scores).mean().median(), 2), 361.9, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now plot it! _(feel free to use plot_learning_curve that we used in the learning notebook, but remember that's custom code)_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_learning_curve(train_sizes_abs, train_scores, test_scores, y_label=\"mse_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-530903dce81a9a1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q4.1) What would you say about the need for more data?**:\n",
    "    \n",
    "- A) If more examples are expensive to come by, it doesn't make sense to gather more data\n",
    "- B) Even if more examples are expensive to come by, we should gather more data\n",
    "- C) Our model doesn't need more than 1000 samples\n",
    "- D) None of the above\n",
    "\n",
    "Enter your answer below wrapped by quotes, for example:\n",
    "\n",
    "```\n",
    "answer_q41 = \"A\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a332c797408db5c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer_q41 = 'A' or 'B' or 'C' \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-aa1f2ef6714f2f00",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer_q41.encode()) == b'QQ=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58e8fe30e15dbc7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That's a wrap! \n",
    "\n",
    "Hopefully you now have a better understanding of how to evaluate models and understand if they are underfitting, overfitting or are just the right fit. On the next unit we'll continue talking about model selection and complexity. See you there! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
