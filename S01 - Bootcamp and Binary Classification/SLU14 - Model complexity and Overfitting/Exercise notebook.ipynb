{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-acae99ed9fc3040b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SLU14: Model complexity and overfitting -- Exercises\n",
    "\n",
    "\n",
    "---\n",
    "*Exercises are graded unless otherwise indicated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43c2ce08e77f6c4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils import expand_dataset\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import json\n",
    "from hashlib import sha1 # just for grading purposes\n",
    "\n",
    "def _hash(obj):\n",
    "    if type(obj) is not str:\n",
    "        obj = json.dumps(obj)\n",
    "    return sha1(obj.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-94483f723ebdd165",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "As a junior data scientist, you are hired by a real estate company to work on their data. The first task they assign to you is to build a predictor of house prices based on a dataset they have. Now, being junior, you don't ask many questions about the purpose of this task and want to apply some of the stuff you've learned as quickly as possible so you dive right into the code.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "For this exercise notebook we'll be working with a house price dataset containing several different features related to the house being sold and the seller. Start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40a9e10bb839be2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function is designed to be used in all the exercises\n",
    "\n",
    "def load_dataset():\n",
    "    # Loads house prices dataset\n",
    "    df = pd.read_csv(\"data/house-prices.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0839bde5b14baf77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_dataset()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5559ca4f8192c31a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1 - complex models \n",
    "\n",
    "\n",
    "After you are briefed on the problem, and feeling already short on time, you decide to start by taking all your data and throwing it at a model. After all, with all the fancy algorithms in data science, it's easy to try and, *who knows*, you might even get a good performance from the start and even improve it further with parameter tuning and all that *fancy data science magic*.\n",
    "\n",
    "\n",
    "<img alt=\"xkcd_shuffle_data_pile\" src=\"media/xkcd_shuffle_data_pile.png\" width=\"300\">\n",
    "\n",
    "\n",
    "### Exercise 1.1 \n",
    "\n",
    "As a first step, to get the data into your model, you notice that you have a lot of categorical values. Knowing that most models need them to be encoded somehow, you keep it simple and decide to convert each categorical column into a set of dummy variables expanding on each category. \n",
    "\n",
    "Implement below the function to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a702af4e4516b05d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def encode_categorical(df): \n",
    "    \"\"\" \n",
    "        Takes a dataframe and encodes all its categorical values\n",
    "        into indicator variables\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the input DataFrame\n",
    "\n",
    "    Returns: df\n",
    "        df (pd.DataFrame): the dataframe with all categorical values encoded as \n",
    "            new variables\n",
    "        \n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return df_encoded \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fd684c5c5c6ec5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Validate your functions in the tests below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0ac3efb218e17336",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_encoded = encode_categorical(df)\n",
    "\n",
    "assert df_encoded.shape == (1460, 1677)\n",
    "assert \"BsmtUnfSF\" in df_encoded.columns\n",
    "assert df_encoded[\"Neighborhood_Gilbert\"][1042] == 0\n",
    "assert df_encoded[\"Condition1_Artery\"][100] == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e026ad969ce5693",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And check your new dataframe below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9693c49de6fb2310",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3374182685cd9be7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "With your prepared data proceed to training a quick baseline. For that purpose you chose to use a decision tree regressor with max_depth of 7 and min_sample_split of 30. \n",
    "\n",
    "\n",
    "Create the function to produce this baseline below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fabf46b5f22e07bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def decision_tree_regression(X, y): \n",
    "    \"\"\"\n",
    "    Fit a DecisionTreeRegressor with `max_depth` of 7, and `min_samples_split` \n",
    "    of 30 on the provided data\n",
    "    \n",
    "    **For reproducibility: Use random state of 42 always**\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): the input DataFrame\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: estimator\n",
    "        estimator (DecisionTreeRegressor): fitted estimator        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return dt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd389b6caf69ba61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Validate the decision tree you've trained on tests below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-afebf2fb162a23f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_encoded = encode_categorical(df)\n",
    "\n",
    "X = df_encoded.drop('SalePrice', axis=1)\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "dt = decision_tree_regression(X, y)\n",
    "\n",
    "assert dt.max_depth == 7\n",
    "assert dt.min_samples_split == 30\n",
    "assert dt.n_features_ == 1676\n",
    "\n",
    "preds = dt.predict(X)\n",
    "\n",
    "np.testing.assert_almost_equal(preds[100], 193.0600, 2)\n",
    "np.testing.assert_almost_equal(preds[1042], 164.8525, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6dc1ea7b2e779581",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.3\n",
    "\n",
    "Finally, you want to evaluate performance on your baseline, and so you create a small function to produce the two regression metrics you want to use: **mean squared error** and **r2 score**.\n",
    "\n",
    "Implement it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc66da8572ab37db",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def regresion_metrics(y, preds): \n",
    "    \"\"\"\n",
    "    Return two regression metrics computed with the provided predictions\n",
    "    `preds` and the true values `y`: mean squared error and R squared\n",
    "    \n",
    "    Args:\n",
    "        preds (pd.Series): the predictions we want to assess\n",
    "        y (pd.Series): the true labels\n",
    "\n",
    "    Returns: mse, r2\n",
    "        mse (float): mean squared error of predicitons     \n",
    "        r2 (float): r-squared value of predicitons        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mse, r2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-02ee0d35ba9ad8a8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_encoded = encode_categorical(df)\n",
    "\n",
    "X = df_encoded.drop('SalePrice', axis=1)\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "dt = decision_tree_regression(X, y)\n",
    "preds = dt.predict(X)\n",
    "\n",
    "mse, r2 = regresion_metrics(y, preds)\n",
    "\n",
    "np.testing.assert_almost_equal(mse, 716.2435, 2)\n",
    "np.testing.assert_almost_equal(r2, 0.8996, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-353e4bf92a8ac52b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Even though we got a pretty big Mean squared error, which can also relate to the big values we're handling, we get a pretty good R-squared value. \n",
    "\n",
    "However, you remember something somebody told you about how every time you evaluate on the data you used to train you lose the ability to tell whether you are overfitting the data you happen to have at hand. So you decide to also apply a held-out test set and check the results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-396c4ee0c309945d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_encoded = encode_categorical(df)\n",
    "print(\"Number of features: {}\".format(len(df_encoded.columns)))\n",
    "\n",
    "X = df_encoded.drop('SalePrice', axis=1)\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "dt = decision_tree_regression(X_train, y_train)\n",
    "\n",
    "preds = dt.predict(X_test)\n",
    "mse, r2 = regresion_metrics(preds, y_test)\n",
    "\n",
    "print(\"Mean Squared Error: {}\".format(mse))\n",
    "print(\"R-squared: {}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6afa25c773f4723a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.4\n",
    "\n",
    "Not so great comparing to training, but also not incredibly bad. Now let's plot the decision tree used for this problem. Implement below a function plotting the decision tree:\n",
    "\n",
    "- Showing all labels\n",
    "- Not showing impurity labels\n",
    "- Do not change the display of ‘values’ and/or ‘samples’ to be proportions\n",
    "\n",
    "Optional (for aesthetics):\n",
    "- Fill the nodes with `filled=True`\n",
    "- Use `rounded=True` to have nice rounded corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bb960c709737d37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_dt_regressor(dt, feature_names): \n",
    "    \"\"\"\n",
    "    Generate plot of decision tree given the estimator and its \n",
    "    feature names. \n",
    "\n",
    "    Args:\n",
    "        dt (DecisionTreeRegressor): fitted decision tree estimator        \n",
    "        feature_names (list): list of feature names\n",
    "\n",
    "    Returns: plot_annots\n",
    "        plot_annots (list): plot annotations     \n",
    "\n",
    "    \"\"\"\n",
    "    plt.subplots(figsize=(100, 20))  # this helps you control the figure size \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return plot_annots \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-75dca58307e23c61",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_encoded = encode_categorical(df)\n",
    "\n",
    "X = df_encoded.drop('SalePrice', axis=1)\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "dt = decision_tree_regression(X, y)\n",
    "tree_plot = plot_dt_regressor(dt, X.columns)\n",
    "\n",
    "sig = inspect.signature(plot_dt_regressor)\n",
    "assert set(sig.parameters.keys()) == {'dt', 'feature_names'}\n",
    "all_text = ''.join([tree_plot[i].get_text() for i in range(len(tree_plot))])\n",
    "assert 'OverallQual' in all_text\n",
    "\n",
    "first_node_feature = tree_plot[0].get_text().split('<')[0].strip()\n",
    "assert _hash(first_node_feature) == '0204560528837e345e60ecc1eccf33d641a30686'\n",
    "assert len(tree_plot) == 107\n",
    "\n",
    "node_10 = tree_plot[10].get_text()\n",
    "assert 'value = 119.154' in node_10\n",
    "assert 'BsmtFinSF1' in node_10, node_10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1c93baad14d8fa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the feature names in the plot against the feature names in the dataframe you used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9815cdee40e3a18e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f3bedaeb0f816e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.5\n",
    "\n",
    "*Hint: To check the decision tree image better, you can save it and open in your own viewer of preference or you double click on the image and it will zoom in)*\n",
    "\n",
    "You look into your process, dataframe and tree with a senior colleague. What is he most likely to tell you? \n",
    "\n",
    "* A) Great job, this seems like a great starter model! \n",
    "* B) You clearly need to prune your decision tree\n",
    "* C) What is a decision tree?\n",
    "* D) Have you looked into the dataset? There's some pretty random features here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-151ee0308225ca10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer = 'A' or 'B' or 'C' or 'D'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8c4aa56c1dacc79",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer.encode()) == b'RA=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ed57216c607b39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Exercise 2 - find the nonsense \n",
    "\n",
    "### Exercise 2.1\n",
    "\n",
    "After this first trial, you proceed with a more methodical approach. There are a few features which should obviously not influence the price and might be just adding unnecessary complexity to the model. In particular, there are two obvious ones that you should be able to identify and discard.\n",
    "\n",
    "Can you find them?\n",
    "\n",
    "Write below which columns (from the initial dataframe) you would remove.\n",
    "\n",
    "\n",
    "_Hint #1: Look into your dataframe after encoding the categorical features. What feature is exploding the number of variables?_  \n",
    "\n",
    "_Hint #2: What other non-categorical column has values that equal the amount of rows we have?_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-870883f72bb82c2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Example answer:\n",
    "# nonsense_features = ['SomeFeature', 'SomeOtherFeature']\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cb2145c781b4aaff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert _hash(sorted(nonsense_features)) == 'a9c3a5038268a67480b94c5206dbb7ef3c54b2d1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c12fb431c1b37aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_removed = df.drop(nonsense_features, axis=1)\n",
    "\n",
    "df_encoded = encode_categorical(df_removed)\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c9aa87a5c1ee101",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Great job! That was a reduction of 1677 to 217 columns, or in terms of model, a reduction from 1677 to 217 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f0c1500d6d4e78c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = df_encoded.drop('SalePrice', axis=1)\n",
    "y = df_encoded['SalePrice']\n",
    "print(\"Number of features: {}\".format(len(df_encoded.columns)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "dt = decision_tree_regression(X_train, y_train)\n",
    "\n",
    "preds = dt.predict(X_train)\n",
    "mse, r2 = regresion_metrics(preds, y_train)\n",
    "\n",
    "print(\"Mean Squared Error (train): {}\".format(mse))\n",
    "print(\"R-squared (train): {}\".format(r2))\n",
    "\n",
    "preds = dt.predict(X_test)\n",
    "mse, r2 = regresion_metrics(preds, y_test)\n",
    "\n",
    "print(\"Mean Squared Error (test): {}\".format(mse))\n",
    "print(\"R-squared (test): {}\".format(r2))\n",
    "\n",
    "tree_plot = plot_dt_regressor(dt, X.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ddd51f3bf6401cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We've dropped more than a thousand features and still get the same performance. Even though, depending on the models, these features may generate more or less noise, this shows how important it is to understand the data before you throw it into your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b232e651991b2c00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "Another way that you can understand the data is by looking into the decision tree image and think about the decisions it's making. If you were to \"guess\" the most important feature given the decision tree, which one would you pick? \n",
    "\n",
    "* A) YearRemodAdd\n",
    "* B) OverallQual\n",
    "* C) WoodDeckSF\n",
    "* D) CentralAir_Y\n",
    "\n",
    "*Hint: To check the decision tree image better, you can save it and open in your own viewer of preference or you double click on the image and it will zoom in)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a5c5c7bca7631b5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# answer = 'A' or 'B' or 'C' or 'D'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f8fc52e6c5a6d470",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert base64.b64encode(answer.encode()) == b'Qg=='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-593bbb96609ba796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "Let's continue on these lines and find other features that do not have importance in this dataset. Use the `mutual_information` method and find all of the features with 0.0 mutual information with the target:\n",
    "\n",
    "*Hint: You have used the `mutual_info_classif` for classification. There is an analogous function for regression in sklearn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-673bad1efe00da17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def zero_mutual_information_features(X, y): \n",
    "    \"\"\"\n",
    "    Use the mutual information method and return all feature names\n",
    "    where the mutual information is zero.\n",
    "\n",
    "    **For reproducibility: Use random state of 42 always**\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): the input DataFrame\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: plot_annots\n",
    "        features (list): list of feature names where mutual information is zero     \n",
    "\n",
    "    \"\"\"\n",
    "    # Compute mutual information of the dataset features\n",
    "    # Hint: use can use the sklearn function here\n",
    "    # mutual_info = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the feature names where the mutual information is zero\n",
    "    # features = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return features \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-364e5f64218f1022",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_removed = df.drop(nonsense_features, axis=1)\n",
    "df_encoded = encode_categorical(df_removed)\n",
    "\n",
    "X = df_encoded.drop('SalePrice', axis=1)\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "bottom_mut_info_features = zero_mutual_information_features(X, y)\n",
    "\n",
    "assert _hash(sorted(bottom_mut_info_features)) == '9c41aa9d5505800207c36f3cb3ecdf9c1d58ea15'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c610211dfba4da23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If we now drop these features we should achieve similar values on our data. Check this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dec4ad830a7f28c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_removed = df.drop(nonsense_features, axis=1)\n",
    "df_encoded = encode_categorical(df_removed).drop(bottom_mut_info_features, axis=1)\n",
    "\n",
    "print(\"Number of features: {}\".format(len(df_encoded.columns)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_encoded.drop('SalePrice', axis=1), \n",
    "    df_encoded['SalePrice'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt = decision_tree_regression(X_train, y_train)\n",
    "\n",
    "preds = dt.predict(X_train)\n",
    "mse, r2 = regresion_metrics(preds, y_train)\n",
    "\n",
    "print(\"Mean Squared Error (train): {}\".format(mse))\n",
    "print(\"R-squared (train): {}\".format(r2))\n",
    "\n",
    "preds = dt.predict(X_test)\n",
    "mse, r2 = regresion_metrics(preds, y_test)\n",
    "\n",
    "print(\"Mean Squared Error (test): {}\".format(mse))\n",
    "print(\"R-squared (test): {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45160bda7cc61c9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this case, our training values are not too different, but our test values decreased. So what's that all about? If these features brought nothing to the table when it came to predicting our target, shouldn't the performance stayed the same?\n",
    "\n",
    "There are a couple of things to take into consideration here as we've been using a fixed model with some random parameters we've chosen to keep it simple:\n",
    "\n",
    "* The parameters may not be adequate and may be leading to overfitting\n",
    "* These \"random\" features may actually have been having a regularization effect on the training, preventing it to overfit too much to other features containing more information\n",
    "* We may just have an unlucky split - methods like using a validation set to optimize parameters and then test on a held-out set or cross-validating with k-fold can help you get more reliable values \n",
    "\n",
    "\n",
    "As stated before not all models suffer equally from adding unnecessary features, but we can't stress enough how important it is that you make sense of your data and avoid delegating the work to your model. Hopefully if you get a real life project you'll start by exploring the features first and then iterating on the model.\n",
    "\n",
    "Let's continue now with some exercises on model specific feature importance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50658e68c72b4705",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 3: model specific feature importances\n",
    "\n",
    "We'll now see some examples of model-specific feature importances. For this purpose, we'll leave aside the decision tree example and focus on two other estimators - linear and non-linear.\n",
    "\n",
    "### Exercise 3.1\n",
    "\n",
    "Start by implementing a LinearRegression model, fit it to our data, and extract the information about feature importances:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66bc63ca7f05d488",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_feat_importances(X, y): \n",
    "    \"\"\"\n",
    "    Fit a linear regression on the provided data and return the top 5 \n",
    "    most important features\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): the input DataFrame\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: top_5_features\n",
    "        top_5_features (list): list of 5 feature names of most importance    \n",
    "    \"\"\"\n",
    "    # Fit a linear regression estimator\n",
    "    # lr = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the top 5 features of the model\n",
    "    # top_5_features = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return top_5_features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c36f925fc0db2ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f51150d6928d10ab",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "\n",
    "# We're constraining the dataset to avoid precision errors\n",
    "X = encode_categorical(\n",
    "    df[\n",
    "        [\n",
    "            'LotArea', \n",
    "            'LotShape', \n",
    "            'LandContour', \n",
    "            'Utilities', \n",
    "            'OverallQual', \n",
    "            '1stFlrSF', \n",
    "            '2ndFlrSF',  \n",
    "            'BsmtUnfSF', \n",
    "            'TotalBsmtSF',\n",
    "            'Utilities',\n",
    "            'LotConfig',\n",
    "            'HouseStyle',\n",
    "            'OverallCond',\n",
    "            'YearBuilt',\n",
    "            'YrSold',\n",
    "            'MoSold',\n",
    "            'GarageCars',\n",
    "            'GarageArea',\n",
    "            'SaleType',\n",
    "            'SaleCondition',\n",
    "            'Fireplaces',\n",
    "            'BldgType',\n",
    "            'BsmtFinSF1',\n",
    "            'BsmtFinSF2',\n",
    "            'Heating',\n",
    "            'CentralAir',\n",
    "            'LowQualFinSF',\n",
    "            'HeatingQC',\n",
    "            'GrLivArea',\n",
    "            'BsmtFullBath'\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "y = df['SalePrice']\n",
    "\n",
    "rs = RobustScaler()  # just scaling, because I'm nice. \n",
    "X_normed = pd.DataFrame(rs.fit_transform(X), columns=X.columns)\n",
    "\n",
    "lr_features = linear_regression_feat_importances(X_normed, y)\n",
    "\n",
    "assert _hash(sorted(lr_features)) == '6b99f61bb095ef08521ac22ea2fd0f53cd7e8f4e'\n",
    "\n",
    "print(\"Top features for linear regression: {}\".format(sorted(lr_features)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff44490adff8c2e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.2\n",
    "\n",
    "Now let's do the same thing for a non-linear model. Train a Random Forest, with the following parameters: \n",
    "\n",
    "* n_estimators = 40 \n",
    "* max_depth = 3\n",
    "* min_samples_split = 20 \n",
    "* random_state = 42\n",
    "* n_jobs = -1  (optional, but speeds things up)\n",
    "\n",
    "Then use it to get feature importances. Use the non-normalized features. As before, get the top 5 features by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-449d3986efce770b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def random_forest_feat_importances(X, y): \n",
    "    \"\"\"\n",
    "    Fit a random forest regressor on the provided data and return the top 5 \n",
    "    most important features. Use 40 estimators, a max depth of 3 and a min\n",
    "    sample split of 20\n",
    "\n",
    "    **For reproducibility: Use random state of 42 always**\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): the input DataFrame\n",
    "        y (pd.Series): the target labels\n",
    "\n",
    "    Returns: top_5_features\n",
    "        top_5_features (list): list of 5 feature names of most importance    \n",
    "    \"\"\"\n",
    "    # Fit a random forest regressor estimator\n",
    "    # lr = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the top 5 features of the model\n",
    "    # top_5_features = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return top_5_features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-573689d4cf79b527",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "df_removed = df.drop(nonsense_features, axis=1)\n",
    "\n",
    "df_encoded = encode_categorical(df_removed)\n",
    "\n",
    "X = df_encoded.drop('SalePrice', axis=1)\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "rf_features = random_forest_feat_importances(X, y)\n",
    "\n",
    "assert _hash(sorted(rf_features)) == 'f4cb12310d33a0a0fc081d61b03ce96dcb98b2e2'\n",
    "\n",
    "print(\"Top features for random forest regressor: {}\".format(sorted(rf_features)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35eb11012b2f36bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Seeing the features of most importance to your model is another way that can help you understand the data better and iterate on the best solution for your problem.\n",
    "\n",
    "By now, you've seen hopefully that feature selection is a useful tool to understand and iterate in your models, in particular it allows us to focus on what matters most and avoid overly complex models. Let's now jump into our final topic - regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c14e3e34972e168",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 4: Regularization\n",
    "\n",
    "Finally, we'll practice the regularization techniques you've learned for regression. Throughout the learning notebooks, you've seen different loss functions that can be used for regularization. We're going to explore L1, but you should be able to follow similar procedures later on for either of the other two.\n",
    "\n",
    "For this purpose, we'll use another house price dataset - the Boston housing dataset. Load the data below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7f3ce0578bae1a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function is designed to be used in the following exercises\n",
    "\n",
    "def load_boston_dataset():\n",
    "    # Loads boston house prices dataset\n",
    "    boston_data = load_boston()\n",
    "    df_boston = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "    df_boston['target'] = pd.Series(boston_data.target)\n",
    "    \n",
    "    return df_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "boston_data = load_boston_dataset()\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-813961f7c6cb0433",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In particular we'll try to check the relation between the LSTAT variable and the target - the house price. Let's start by using polynomial features and fitting a linear regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9aedc4e15c143910",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_boston = load_boston_dataset()\n",
    "df_train = df_boston.set_index('LSTAT', drop=False).sort_index()\n",
    "\n",
    "X = df_train[['LSTAT']]\n",
    "y = df_train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=111)\n",
    "\n",
    "X_poly = expand_dataset(X_train, n_expansions=10, feature_name='LSTAT')\n",
    "X_poly_test = expand_dataset(X_test, n_expansions=10, feature_name='LSTAT')\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_poly, y_train)\n",
    "preds_train = lr.predict(X_poly)\n",
    "preds_test = lr.predict(X_poly_test)\n",
    "\n",
    "X_train_plot, preds_plot = zip(*sorted(zip(X_train.values, preds_train)))\n",
    "\n",
    "mse = mean_squared_error(y_train, preds_train)\n",
    "print(\"Mean squared error (train): {}\".format(mse))\n",
    "\n",
    "r2 = r2_score(y_train, preds_train)\n",
    "print(\"R2 score (test): {}\".format(r2))\n",
    "\n",
    "mse = mean_squared_error(y_test, preds_test)\n",
    "print(\"Mean squared error (test): {}\".format(mse))\n",
    "\n",
    "r2 = r2_score(y_test, preds_test)\n",
    "print(\"R2 score (test): {}\".format(r2))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_train, y_train, c='orange', s=5, label=\"Original data\")\n",
    "plt.plot(X_train_plot, preds_plot, c='blue', label=\"Polynomial Regression\")\n",
    "plt.title(\"Polynomial regression with 10 degrees\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('target')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-584fb2271ef33786",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Seems a bit overfitted. \n",
    "\n",
    "### Exercise 4.1\n",
    "\n",
    "So let's try some regularization techniques. First, remember the L1 loss:\n",
    "\n",
    "$$J_{L_1} = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_1 \\sum_{k=1}^K \\left|\\beta_k\\right|$$\n",
    "\n",
    "Start by implementing it in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb420f403428e074",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def l1_loss(y, y_hat, betas, lamb):\n",
    "    \"\"\"\n",
    "    Implement the L1-loss function \n",
    "\n",
    "    Args: \n",
    "        y (np.array): numpy array with shape (num_observations,)\n",
    "            The targets.\n",
    "        y_hat (np.array): numpy array with shape (num_observations,)\n",
    "            The predictions.\n",
    "        betas (np.array): numpy array with shape (num_features+1,)\n",
    "            The parameters of your regression model. \n",
    "            The first value is the intercept and the \n",
    "            remaining ones are the feature coefficients.\n",
    "        lamb (float): The strength of the L1 regularizer.\n",
    "            \n",
    "    Returns:\n",
    "        L (float): loss with L1-regularization\n",
    "    \"\"\"\n",
    "        \n",
    "    # Compute the mean squared error loss part of the general loss function.\n",
    "    # Hint: use can use the sklearn function here\n",
    "    # mse = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute the L1 part of the general loss function.\n",
    "    # l1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute the total loss by combining the parts.\n",
    "    # L = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd918ab729f032e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your loss function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-018e3a9a0a954d6d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "loss1 = l1_loss(\n",
    "    np.array([0.89305005, 0.53765939, 0.58379047, 0.39355386, 0.30221551, 0.18592352, 0.4025869 , 0.63137406]), \n",
    "    np.array([0.23556499, 0.5899186 , 0.77827108, 0.57362455, 0.04727258, 0.99407257, 0.21856487, 0.25006812]), \n",
    "    np.array([0.45836509, 0.78367697, 0.6914712]), \n",
    "    1\n",
    ")\n",
    "np.testing.assert_almost_equal(loss1, 1.6504, 3)\n",
    "\n",
    "loss1 = l1_loss(\n",
    "    np.array([0.89305005, 0.53765939, 0.58379047, 0.39355386, 0.30221551, 0.18592352, 0.4025869 , 0.63137406]), \n",
    "    np.array([0.23556499, 0.5899186 , 0.77827108, 0.57362455, 0.04727258, 0.99407257, 0.21856487, 0.25006812]), \n",
    "    np.array([0.45836509, 0.78367697, 0.6914712]), \n",
    "    6\n",
    ")\n",
    "np.testing.assert_almost_equal(loss1, 9.0262, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c684df5a5994a203",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's apply it to our subset of the data and the linear regression we've trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-942d0c8ea14de28d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_boston = load_boston_dataset()\n",
    "df_train = df_boston.set_index('LSTAT', drop=False).sort_index()\n",
    "\n",
    "X = df_train[['LSTAT']]\n",
    "y = df_train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=111)\n",
    "\n",
    "X_poly = expand_dataset(X_train, n_expansions=10, feature_name='LSTAT')\n",
    "X_poly_test = expand_dataset(X_test, n_expansions=10, feature_name='LSTAT')\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_poly, y_train)\n",
    "preds_train = lr.predict(X_poly)\n",
    "preds_test = lr.predict(X_poly_test)\n",
    "\n",
    "betas = np.append(lr.intercept_, lr.coef_)\n",
    "\n",
    "l1_1 = l1_loss(y_train, preds_train, betas, 1)\n",
    "print(\"L1 train (beta=1): {}\".format(l1_1))\n",
    "\n",
    "l1_5 = l1_loss(y_train, preds_train, betas, 5)\n",
    "print(\"L1 train (beta=5): {}\".format(l1_5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5510f708d68a814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It shouldn't surprise you that the loss gets larger by adding the regularization factor, as we let the linear regression train with focus solely on the mean squared error loss and our coefficients are probably somewhat large. Plot them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(lr.coef_)), [abs(coef) for coef in lr.coef_], marker='o', markerfacecolor='r')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Coef. magnitude')\n",
    "plt.title(\"Coefficient magnitude (no regularization)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e59e830e916c2638",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "Now let's see what would happen if you would have trained using this regularized loss from the start. \n",
    "\n",
    "Implement below the function that fits a regressor using the l1_loss (*Hint: sklearn offers and out of the box estimator for this that you should use*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1391c046505e53b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def l1_loss_lr(X, y, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Fit a linear regressor using the L1-loss with normalize=True and return it \n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): the input DataFrame\n",
    "        y (pd.Series): the target labels\n",
    "        alpha (float): the L1 coefficient\n",
    "\n",
    "    Returns: l1_estimator\n",
    "        l1_estimator: fitted estimator        \n",
    "    \"\"\"\n",
    "        \n",
    "    # lasso_lr = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return l1_estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13521cb110889327",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-abf3ea147028fab5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_boston = load_boston_dataset()\n",
    "df_train = df_boston.set_index('LSTAT', drop=False).sort_index()\n",
    "\n",
    "X = df_train[['LSTAT']]\n",
    "y = df_train['target']\n",
    "\n",
    "X_poly = expand_dataset(X, n_expansions=10, feature_name='LSTAT')\n",
    "lr_l1 = l1_loss_lr(X_poly, y, alpha=0.05)\n",
    "\n",
    "preds_train = lr_l1.predict(X_poly)\n",
    "\n",
    "np.testing.assert_almost_equal(preds_train[1], 31.0376, 2)\n",
    "np.testing.assert_almost_equal(preds_train[41], 29.0328, 2)\n",
    "np.testing.assert_almost_equal(preds_train[232], 24.3181, 2)\n",
    "np.testing.assert_almost_equal(preds_train[312], 21.7428, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-83ba79130eb4d3c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll apply this regressor to your data in the same way we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fdf246283ba98b29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_boston = load_boston_dataset()\n",
    "df_train = df_boston.set_index('LSTAT', drop=False).sort_index()\n",
    "\n",
    "X = df_train[['LSTAT']]\n",
    "y = df_train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=111)\n",
    "\n",
    "X_poly = expand_dataset(X_train, n_expansions=10, feature_name='LSTAT')\n",
    "X_poly_test = expand_dataset(X_test, n_expansions=10, feature_name='LSTAT')\n",
    "\n",
    "lr_l1 = l1_loss_lr(X_poly, y_train, alpha=0.0001)\n",
    "preds_train = lr_l1.predict(X_poly)\n",
    "preds_test = lr_l1.predict(X_poly_test)\n",
    "\n",
    "betas = np.append(lr_l1.intercept_, lr_l1.coef_)\n",
    "\n",
    "mse = mean_squared_error(y_train, preds_train)\n",
    "print(\"Mean squared error (train): {}\".format(mse))\n",
    "\n",
    "r2 = r2_score(y_train, preds_train)\n",
    "print(\"R2 score (test): {}\".format(r2))\n",
    "\n",
    "mse = mean_squared_error(y_test, preds_test)\n",
    "print(\"Mean squared error (test): {}\".format(mse))\n",
    "\n",
    "r2 = r2_score(y_test, preds_test)\n",
    "print(\"R2 score (test): {}\".format(r2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-035edae0609a76fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We get much more stable values! Great! Finally, we can plot the coefficients for this model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6da590289d183b8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(lr_l1.coef_)), [abs(coef) for coef in lr_l1.coef_], marker='o', markerfacecolor='r')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Coef. magnitude')\n",
    "plt.title(\"Coefficient magnitude (with regularization)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-708c3ba716f7ce1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "They are much lower, which typically can help prevent overfitting, as you've seen before.\n",
    "\n",
    "And that's a wrap!\n",
    "\n",
    "Congratulations on getting to the end of this exercise notebook. We hope you understood the importance of looking into your data and understanding it, making informed decisions on what features to use and what features to discard. Additionally, you should always keep in mind that starting simple and building from there is a much better approach than choosing overly complex approaches that will become harder to debug.\n",
    "\n",
    "\n",
    "See you on the next unit! \n",
    "\n",
    "</br> \n",
    "\n",
    "<img alt=\"affordable_house\" src=\"media/affordable-house.jpg\" width=\"500\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
