{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46d843154dcd7d0f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SLU10: Metrics for Classification -- Exercises\n",
    "\n",
    "In this notebook we'll put in practice all the metrics you've learned for classification:\n",
    "\n",
    "- Understanding the problem with accuracy\n",
    "- Understanding FP, TP, FN, TN\n",
    "- Understanding precision, recall\n",
    "- Understanding the ROC Curve \n",
    "- Understanding AUROC\n",
    "- Which model is better for a particular circumstance\n",
    "- Using these metrics in day to day \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8d860e3a9483163",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "import numpy as np\n",
    "from utils import plot_confusion_matrix, hash_answer\n",
    "from matplotlib import pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe9b148043106aa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function is designed to be used in all the exercises\n",
    "\n",
    "def load_data():\n",
    "    # Loads classifier probabilities dataframe\n",
    "    df = pd.read_csv(\"data/classifier_prediction_scores.csv\").rename(columns={'scores': 'probas'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f89a55ca33111154",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Useful information\n",
    "\n",
    "In this exercise we have the following dataset: \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>probas</th>      <th>target</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.288467</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0.255047</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>0.201017</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>0.729307</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>0.148288</td>      <td>0</td>    </tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d34c4723b41a782",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1 - Understanding the problem with Accuracy \n",
    "\n",
    "We will perform the following steps without using scikit: \n",
    "\n",
    "1. Create a column called \"predicted at 0.5 threshold\", which will be:\n",
    " - 0 if probas is smaller than 0.5  \n",
    " - 1 otherwise (larger or equal to 0.5)   \n",
    "2. Calculate the accuracy of the predictions (feel free to use a support column called \"correct prediction\")\n",
    "3. Calculate what the accuracy would have been if you had simply predicted 0 every time \n",
    "\n",
    "Start by running the cell below, which implements a small function to compute your prediction from the probas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c181744c80d2ac0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def threshold_probas(proba, threshold=.5): \n",
    "    if proba >= threshold:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f34a326a5fa50173",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now implement below a function to compute the accuracy of a dataframe which assumes:\n",
    "\n",
    "- Dataframe contains a `target` column with the true labels\n",
    "- Dataframe contains a `prediction` column with the model prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72ba6d5cbf2be8e0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    \"\"\"\n",
    "        Given a dataframe with `prediction` and `target` columns, \n",
    "        compute the accuracy metric\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the dataframe, with `prediction` and `target` columns\n",
    "\n",
    "    Returns: accuracy\n",
    "        accuracy (float): accuracy measuree     \n",
    "    \"\"\"    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-26325384c86158fa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame({\"target\": np.array([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0])})\n",
    "\n",
    "df_1_all_zeros = df_1.copy()\n",
    "df_1_all_zeros[\"prediction\"] = 0 \n",
    "np.testing.assert_almost_equal(accuracy(df_1_all_zeros), 0.4545, 2)\n",
    "\n",
    "df_1_all_ones = df_1.copy()\n",
    "df_1_all_ones[\"prediction\"] = 1\n",
    "np.testing.assert_almost_equal(accuracy(df_1_all_ones), 0.54545, 2)\n",
    "\n",
    "df_2 = pd.DataFrame({\"target\": np.array([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])})\n",
    "\n",
    "df_2_all_zeros = df_2.copy()\n",
    "df_2_all_zeros[\"prediction\"] = 0 \n",
    "np.testing.assert_almost_equal(accuracy(df_2_all_zeros), 0.2727, 2)\n",
    "\n",
    "df_2_all_ones = df_2.copy()\n",
    "df_2_all_ones[\"prediction\"] = 1\n",
    "np.testing.assert_almost_equal(accuracy(df_2_all_ones), 0.7272, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-56fb3b7d3d008271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's apply this to our actual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff217b96e0d0697f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df[\"prediction\"] = df['probas'].map(threshold_probas)\n",
    "print(f\"Accuracy of predictions: {accuracy(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b8c7341fba78da8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's see what would happen if we just predicted zero (majority class) or one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df9ea34d6d402024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df[\"prediction\"] = 0\n",
    "print(f\"Accuracy of predicting always zero: {accuracy(df)}\")\n",
    "\n",
    "\n",
    "df[\"prediction\"] = 1\n",
    "print(f\"Accuracy of predicting always one: {accuracy(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76914a3b73cc4208",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "```\n",
    "Predictions accuracy:               0.8304 \n",
    "Accuracy of predicting always zero: 0.8336\n",
    "Accuracy of predicting always one: 0.1664\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67b13c4f54a6a31b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you see, the accuracy can be misleading and if we just focused on that we could end up with a model that just predicts all zero values. Let's move on to the techniques you've seen in the learning notebook.\n",
    "\n",
    "We'll start with the confusion matrix and the insights it gives us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d025a0824fb08ef3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Understanding main metrics at a particular threshold \n",
    "\n",
    "We'll now look into confusion matrices. For this purpose we'll use a threshold of 0.3 and plot the confusion matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f454e07e9cb07fa7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df[\"prediction\"] = df['probas'].map(lambda a: threshold_probas(a, threshold=0.3))\n",
    "\n",
    "plot_confusion_matrix(df[\"target\"], df[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6922da30d097e4be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "For the following confusion matrix, calculate (manually) the number of:\n",
    "\n",
    "- False Positives    (FP)\n",
    "- True Positives     (TP)\n",
    "- False Negatives    (FN)\n",
    "- True Negatives     (TN)\n",
    "\n",
    "Note: you can hardcode the number in this one :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bfc98777980f60ed",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def my_confusion_matrix_analysis():\n",
    "    \"\"\"\n",
    "    Analyse the confusion matrix above and write down the numbers for:\n",
    "        - `true_positives`\n",
    "        - `true_negatives`\n",
    "        - `false_positives`\n",
    "        - `false_negatives`\n",
    "    \n",
    "    Returns: true_positives, true_negatives, false_positives, false_negatives\n",
    "        true_positives (int): number of true positives     \n",
    "        true_negatives (int): number of true negatives     \n",
    "        false_positives (int): number of false positives     \n",
    "        false_negatives (int): number of false positives     \n",
    "\n",
    "    \"\"\"    \n",
    "\n",
    "    # true_positives = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # true_negatives = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # false_positives = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # false_negatives = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return true_positives, true_negatives, false_positives, false_negatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f7dda939ca8ad6e4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "TP, TN, FP, FN = my_confusion_matrix_analysis()\n",
    "\n",
    "assert hash_answer(TP) == '2fca346db656187102ce806ac732e06a62df0dbb2829e511a770556d398e1a6e'\n",
    "assert hash_answer(TN) == '6d094db49a6e224278dd87d28835b66af6d2db257522e22c105b829cf368af98' \n",
    "assert hash_answer(FP) == '29db0c6782dbd5000559ef4d9e953e300e2b479eed26d887ef3f92b921c06a67'\n",
    "assert hash_answer(FN) == '1d0ebea552eb43d0b1e1561f6de8ae92e3de7f1abec52399244d1caed7dbdfa6'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6fc22e92299e8f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Great job! Now that you've identified what is each measure, we'll now compute the metrics you've learned about. Implement below each metric separately in a function: \n",
    "\n",
    "- True Positive Rate  (TPR)\n",
    "- False Positive Rate (FPR)\n",
    "- Recall\n",
    "- Precision\n",
    "\n",
    "\n",
    "Note that we pass the same input to all metric functions but **each function may discard some of the inputs**. Pay attention to the formulas given and implement each accordingly:\n",
    "\n",
    "\n",
    "#### 1. False Positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-516897dd70339b47",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def false_positive_rate(TP, TN, FP, FN):\n",
    "    \"\"\"\n",
    "    Given the true_positives, true_negatives, false_positives and false_negatives\n",
    "    compute the false positive rate\n",
    "    \n",
    "    Args:\n",
    "        TP (int): number of true positives     \n",
    "        TN (int): number of true negatives     \n",
    "        FP (int): number of false positives     \n",
    "        FN (int): number of false positives     \n",
    "\n",
    "\n",
    "    Returns: false_positive_rate\n",
    "        false_positive_rate (float): false positive rate     \n",
    "    \"\"\"    \n",
    "\n",
    "    # false_positive_rate = ...  (a.k.a False Alarm Rate!)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return false_positive_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d472dab689d7cb50",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(false_positive_rate(10, 40, 13, 42), 0.2452, 2)\n",
    "np.testing.assert_almost_equal(false_positive_rate(24, 42, 4, 64), 0.0869, 2)\n",
    "np.testing.assert_almost_equal(false_positive_rate(64, 26, 64, 15), 0.7111, 2)\n",
    "np.testing.assert_almost_equal(false_positive_rate(17, 15, 1, 3), 0.0625, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3b34bc1323ecb94f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2. True Positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a740e1c5ee69421a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "def true_positive_rate(TP, TN, FP, FN):\n",
    "    \"\"\"\n",
    "    Given the true_positives, true_negatives, false_positives and false_negatives\n",
    "    compute the true positive rate\n",
    "    \n",
    "    Args:\n",
    "        TP (int): number of true positives     \n",
    "        TN (int): number of true negatives     \n",
    "        FP (int): number of false positives     \n",
    "        FN (int): number of false positives     \n",
    "\n",
    "\n",
    "    Returns: true_positive_rate\n",
    "        true_positive_rate (float): true positive rate     \n",
    "    \"\"\"    \n",
    "\n",
    "    # true_positive_rate = ... \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return true_positive_rate\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-caa8049e86187fa1",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(true_positive_rate(10, 40, 13, 42), 0.1923, 2)\n",
    "np.testing.assert_almost_equal(true_positive_rate(24, 42, 4, 64), 0.2727, 2)\n",
    "np.testing.assert_almost_equal(true_positive_rate(64, 26, 64, 15), 0.8101, 2)\n",
    "np.testing.assert_almost_equal(true_positive_rate(17, 15, 1, 3), 0.8500, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-810deeec6e16fe7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 3. Precision metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c66a7234a25f7dc8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def precision_metric(TP, TN, FP, FN):\n",
    "    \"\"\"\n",
    "    Given the true_positives, true_negatives, false_positives and false_negatives\n",
    "    compute the precision\n",
    "    \n",
    "    Args:\n",
    "        TP (int): number of true positives     \n",
    "        TN (int): number of true negatives     \n",
    "        FP (int): number of false positives     \n",
    "        FN (int): number of false positives     \n",
    "\n",
    "\n",
    "    Returns: precision\n",
    "        precision (float): precision score     \n",
    "    \"\"\"    \n",
    "    # precision = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return precision\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a10585e0d5167a4d",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(precision_metric(10, 40, 13, 42), 0.4348, 2)\n",
    "np.testing.assert_almost_equal(precision_metric(24, 42, 4, 64), 0.8571, 2)\n",
    "np.testing.assert_almost_equal(precision_metric(64, 26, 64, 15), 0.5000, 2)\n",
    "np.testing.assert_almost_equal(precision_metric(17, 15, 1, 3), 0.94444, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0ae2181566de687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 4. Recall metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cf0fcc674b2d58f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def recall_metric(TP, TN, FP, FN):\n",
    "    \"\"\"\n",
    "    Given the true_positives, true_negatives, false_positives and false_negatives\n",
    "    compute the recall\n",
    "    \n",
    "    Args:\n",
    "        TP (int): number of true positives     \n",
    "        TN (int): number of true negatives     \n",
    "        FP (int): number of false positives     \n",
    "        FN (int): number of false positives     \n",
    "\n",
    "\n",
    "    Returns: recall\n",
    "        recall (float): recall score     \n",
    "    \"\"\"    \n",
    "    # recall = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-51b114e42c819d0f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(recall_metric(10, 40, 13, 42), 0.1923, 2)\n",
    "np.testing.assert_almost_equal(recall_metric(24, 42, 4, 64), 0.2727, 2)\n",
    "np.testing.assert_almost_equal(recall_metric(64, 26, 64, 15), 0.8101, 2)\n",
    "np.testing.assert_almost_equal(recall_metric(17, 15, 1, 3), 0.8500, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b3bed38e3cf2ee1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally let's apply our metrics to the dataframe we had. We'll compute these for the threshold of 0.3, which corresponds to the confusion matrix presented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0312a1da01fce",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "FPR = false_positive_rate(TP, TN, FP, FN)\n",
    "TPR = true_positive_rate(TP, TN, FP, FN)\n",
    "precision = precision_metric(TP, TN, FP, FN)\n",
    "recall = recall_metric(TP, TN, FP, FN)\n",
    "\n",
    "print('True Positives:       %0.0f' % TP)\n",
    "print('True Negatives:       %0.0f' % TN)\n",
    "print('False Positives:      %0.0f' % FP)\n",
    "print('False Negatives:      %0.0f' % FN)\n",
    "print('False Positive rate:  %0.2f' % FPR)\n",
    "print('True Positive rate:   %0.2f' % TPR)\n",
    "print('precision:            %0.2f' % precision)\n",
    "print('recall:               %0.2f' % recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1a3d33376c70afb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "    True Positives:       <not telling you!>\n",
    "    True Negatives:       <not telling you!>\n",
    "    False Positives:      <not telling you!>\n",
    "    False Negatives:      <not telling you!>\n",
    "    False Positive rate:  0.09\n",
    "    True Positive rate:   0.26\n",
    "    precision:            0.36\n",
    "    recall:               0.26\n",
    "    \n",
    "_Note: your `True Positive Rate` and `Recall` should be the same because... ehm... (check their formula!)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e542b0c2502309e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 3: model selection \n",
    "\n",
    "Consider the following two confusion matrixes: \n",
    "\n",
    "<img src=\"media/conf_mats.png\" alt=\"drawing\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b50fa7cfe6ed2f83",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# which option (A or B) is better if you are evaluating the performance of a judge over his career? \n",
    "# In this situation: \n",
    "# - 1 is guilty\n",
    "# - 0 is innocent \n",
    "# - predicting 1 sends people to jail\n",
    "# - predicting 0 sends people home free\n",
    "\n",
    "# best_option_for_a_judge = ... (\"A\" or \"B\")\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# which option (A or B) is best, if this is a model for screening for cancer? \n",
    "# In this situation: \n",
    "# - 1 is cancer\n",
    "# - 0 is healthy\n",
    "# - predicting 1 sends people to a cancer screening\n",
    "# - predicting 0 sends people home \n",
    "\n",
    "# best_option_for_cancer_screening = ... (\"A\" or \"B\")\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f6e108311857c10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hash_answer(best_option_for_cancer_screening.lower()) == '3e23e8160039594a33894f6564e1b1348bbd7a0088d42c4acb73eeaed59c009d' \n",
    "assert hash_answer(best_option_for_a_judge.lower()) == 'ca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0fbe84422a1518aa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: Understanding the ROC Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0e4693c249ada78",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# for your convenience, we have already written most of this.\n",
    "# most of this exercise is understanding what is going on, and completing the last functions :)\n",
    "\n",
    "def threshold_probas(proba, threshold): \n",
    "    # returns the thresholded prediction for a single observation \n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     proba (float): a scores or probability, between 0.0 and 1.0\n",
    "    #     threshold (float): a number between 0.0 and 1.0    \n",
    "    # -----------------------------------------------------\n",
    "    # example: threshold_probas(.9, .5) = 1\n",
    "    if proba >= threshold:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0 \n",
    "    \n",
    "def get_predictions(probas, threshold):\n",
    "    # returns the thresholded predictions \n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     probas (pd.Series): a series of floats (scores or probabilities) between 0.0 and 1.0\n",
    "    #     threshold (float): a number between 0.0 and 1.0    \n",
    "    # -----------------------------------------------------\n",
    "    # example: get_predictions([.1, .2, .5], .4) = [0, 0, 1]\n",
    "    # -----------------------------------------------------\n",
    "    return probas.map(lambda x: threshold_probas(x, threshold))\n",
    "\n",
    "def calculate_number_of_true_positives(predictions, target):\n",
    "    # calculates the number of true positives \n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "    #     target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    # -----------------------------------------------------\n",
    "    # example: calculate_number_of_true_positives([[0, 1, 0]], [0, 1, 1]) = 1 \n",
    "    # -----------------------------------------------------\n",
    "    positive_predictions_indexes = (predictions == 1)\n",
    "    true_positives = predictions.loc[positive_predictions_indexes] == target[positive_predictions_indexes]\n",
    "    return true_positives.sum()\n",
    "    \n",
    "def calculate_number_of_false_positives(predictions, target): \n",
    "    # calculates the number of false positives \n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "    #     target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    # -----------------------------------------------------\n",
    "    # example: calculate_number_of_false_positives([[0, 1, 0]], [0, 1, 1]) = 0\n",
    "    # -----------------------------------------------------\n",
    "    positive_predictions_indexes = (predictions == 1)\n",
    "    false_positives = predictions.loc[positive_predictions_indexes] != target[positive_predictions_indexes]\n",
    "    return false_positives.sum()\n",
    "    \n",
    "def calculate_number_of_true_negatives(predictions, target): \n",
    "    # calculates the number of true_negatives\n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "    #     target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    # -----------------------------------------------------\n",
    "    # example: calculate_number_of_true_negatives([[0, 1, 0]], [0, 1, 1]) = 1 \n",
    "    # -----------------------------------------------------\n",
    "    negative_predictions_indexes = (predictions == 0)\n",
    "    true_negatives = predictions.loc[negative_predictions_indexes] == target[negative_predictions_indexes]\n",
    "    return true_negatives.sum()\n",
    "\n",
    "def calculate_number_of_false_negatives(predictions, target): \n",
    "    # calculates the number of false_negatives\n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "    #     target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    # -----------------------------------------------------\n",
    "    # example: calculate_number_of_false_negatives([[0, 1, 0]], [0, 1, 1]) = 1 \n",
    "    # -----------------------------------------------------\n",
    "    negative_predictions_indexes = (predictions == 0)\n",
    "    false_negatives = predictions.loc[negative_predictions_indexes] != target[negative_predictions_indexes]\n",
    "    return false_negatives.sum()\n",
    "\n",
    "def calculate_true_positives_rate(predictions, target):\n",
    "    # calculate the true positive rate\n",
    "    # hint: we have defined most things above, so 2 of the lines are just function calls \n",
    "    # true_positive_rate = ... (~ 3 lines)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return true_positive_rate\n",
    "\n",
    "def calculate_false_positives_rate(predictions, target):\n",
    "    # calculate the false positive rate \n",
    "    # hint: we have defined most things above, so 2 of the lines are just function calls \n",
    "    # false_positive_rate = ... (~ 3 lines)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return false_positive_rate\n",
    "\n",
    "def hand_made_roc_curve(probas, target, list_of_thresholds):\n",
    "    \n",
    "    TPRs = {}\n",
    "    FPRs = {}\n",
    "    \n",
    "    # for each threshold, get the predictions, and calculate the TPR and FPR\n",
    "    # hint: you already defined everything above, so each line should be just a function call   \n",
    "    for threshold in list_of_thresholds:\n",
    "        # predictions = \n",
    "        # TPRs[threshold] = ...\n",
    "        # FPRs[threshold] = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return {'True Positive Rate': TPRs, 'False Positive Rate': FPRs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5b547305337b81d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "# generating a space of thresholds (we'll be more \"dense near zero\")\n",
    "list_of_thresholds = np.linspace(0, 10) ** 2 / 100\n",
    "\n",
    "# using the function you have just created \n",
    "hand_made_roc = hand_made_roc_curve(df['probas'], df['target'], list_of_thresholds)\n",
    "\n",
    "# just making a dataframe to pretty things up\n",
    "hand_made_roc_df = pd.DataFrame({'False Positive Rate': hand_made_roc['False Positive Rate'], \n",
    "                        'True Positive Rate': hand_made_roc['True Positive Rate']})\n",
    "\n",
    "# naming the index\n",
    "hand_made_roc_df.index.name = 'Thresholds'\n",
    "\n",
    "# plotting the ROC curve \n",
    "hand_made_roc_df.set_index('False Positive Rate').plot();\n",
    "plt.show()\n",
    "\n",
    "# displaying the ROC curve \n",
    "display(hand_made_roc_df.iloc[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25a6f9756a2a30ec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>fpr</th>      <th>tpr</th>    </tr>    <tr>      <th>Thresholds</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>0.041649</th>      <td>0.964491</td>      <td>0.995192</td>    </tr>    <tr>      <th>0.050396</th>      <td>0.924184</td>      <td>0.995192</td>    </tr>    <tr>      <th>0.059975</th>      <td>0.879079</td>      <td>0.990385</td>    </tr>    <tr>      <th>0.070387</th>      <td>0.833013</td>      <td>0.975962</td>    </tr>    <tr>      <th>0.081633</th>      <td>0.786948</td>      <td>0.956731</td>    </tr>    <tr>      <th>0.093711</th>      <td>0.741843</td>      <td>0.942308</td>    </tr>    <tr>      <th>0.106622</th>      <td>0.688100</td>      <td>0.899038</td>    </tr>    <tr>      <th>0.120367</th>      <td>0.619002</td>      <td>0.826923</td>    </tr>    <tr>      <th>0.134944</th>      <td>0.544146</td>      <td>0.783654</td>    </tr>    <tr>      <th>0.150354</th>      <td>0.467370</td>      <td>0.745192</td>    </tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5d25008fa21ed0ab",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "hand_made_roc = hand_made_roc_curve(df['probas'], df['target'], list_of_thresholds)\n",
    "hand_made_roc_df = pd.DataFrame({'False Positive Rate': hand_made_roc['False Positive Rate'], \n",
    "                        'True Positive Rate': hand_made_roc['True Positive Rate']})\n",
    "\n",
    "assert math.isclose(hand_made_roc_df['False Positive Rate'].iloc[15], 0.741, abs_tol=.001)\n",
    "assert math.isclose(hand_made_roc_df['True Positive Rate'].sum(), 23.85, abs_tol=.01)\n",
    "assert hand_made_roc_df.shape == (50, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-56dc6b11c62e5e0a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5: Understanding AUROC (Area under the ROC Curve)\n",
    "\n",
    "Great! You (hopefully) made a ROC curve! \n",
    "\n",
    "We also established that an important metric was the AUROC, which was, quite simply, the area under the curve. \n",
    "\n",
    "Instead of using super-sophisticated methods for calculating areas, we have just printed some rectangles on top of (your!) ROC curve and printed the heights. \n",
    "\n",
    "All rectangles have a side of 0.1.\n",
    "\n",
    "<img src=\"media/roc_with_squares.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cf1dc7dc8eaea8b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# what is the area under the ROC curve? (approximately, using the rectangles)\n",
    "\n",
    "# approximate_area_under_the_ROC_curve = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d42f7040126fd74b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hash_answer(round(approximate_area_under_the_ROC_curve, 3)\n",
    "                  ) == 'af83e05e96b869cab105fdf201163c84e8a953ae17c50db344c3f58d7dd74a0b'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3422c5927ff8fbf9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 6: The joy of scikit \n",
    "Ok, now you (finally) get to use scikit :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-525a214e5b698b83",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "df['prediction'] = get_predictions(df['probas'], threshold=.3)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1c1d6a2886ee82c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Start by implementing some basic metrics - accuracy, precision and recall - using scikitlearn's functions instead of implementing from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67c2e8ef543f95ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_score_sklearn(predictions, target):\n",
    "    \"\"\"\n",
    "    Given the predicted and true labels, return the accuracy using\n",
    "    sklearn\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series or np.array): classifier predictions     \n",
    "        target (pd.Series or np.array): true labels     \n",
    "\n",
    "    Returns: accuracy\n",
    "        accuracy (float): accuracy score     \n",
    "    \"\"\"    \n",
    "    # NOTE: Even though this is not a good programming practice, just \n",
    "    #       for the sake of the exercise, ensure you import the right \n",
    "    #       function within this code:\n",
    "    \n",
    "    # Hint:\n",
    "    # from ...\n",
    "    # precision = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def precision_metric_sklearn(predictions, target):\n",
    "    \"\"\"\n",
    "    Given the predicted and true labels, return the precision using\n",
    "    sklearn\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series or np.array): classifier predictions     \n",
    "        target (pd.Series or np.array): true labels     \n",
    "\n",
    "    Returns: precision\n",
    "        precision (float): precision score     \n",
    "    \"\"\"    \n",
    "    # NOTE: Even though this is not a good programming practice, just \n",
    "    #       for the sake of the exercise, ensure you import the right \n",
    "    #       function within this code:\n",
    "    \n",
    "    # Hint:\n",
    "    # from ? import ...\n",
    "    # precision = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall_score_sklearn(predictions, target):\n",
    "    \"\"\"\n",
    "    Given the predicted and true labels, return the recall using\n",
    "    sklearn\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series or np.array): classifier predictions     \n",
    "        target (pd.Series or np.array): true labels     \n",
    "\n",
    "    Returns: recall\n",
    "        recall (float): recall score     \n",
    "    \"\"\"    \n",
    "    # NOTE: Even though this is not a good programming practice, just \n",
    "    #       for the sake of the exercise, ensure you import the right \n",
    "    #       function within this code:\n",
    "    \n",
    "    # Hint:\n",
    "    # from ? import ...\n",
    "    # precision = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-78d7b23cb6126162",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_1 = df.iloc[100:200].copy()\n",
    "\n",
    "np.testing.assert_almost_equal(accuracy_score_sklearn(df_1['prediction'], df_1['target']), 0.7400, 2)\n",
    "np.testing.assert_almost_equal(precision_metric_sklearn(df_1['prediction'], df_1['target']), 0.2778, 2)\n",
    "np.testing.assert_almost_equal(recall_score_sklearn(df_1['prediction'], df_1['target']), 0.2778, 2)\n",
    "\n",
    "df_2 = df.iloc[0:50].copy()\n",
    "\n",
    "np.testing.assert_almost_equal(accuracy_score_sklearn(df_2['prediction'], df_2['target']), 0.7600, 2)\n",
    "np.testing.assert_almost_equal(precision_metric_sklearn(df_2['prediction'], df_2['target']), 0.5000, 2)\n",
    "np.testing.assert_almost_equal(recall_score_sklearn(df_2['prediction'], df_2['target']), 0.2500, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7233a3932138422b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's check the metrics for the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a3e823482a3aab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy_score_sklearn(df['prediction'], df['target'])}\")\n",
    "print(f\"Precision: {precision_metric_sklearn(df['prediction'], df['target'])}\")\n",
    "print(f\"Recall: {recall_score_sklearn(df['prediction'], df['target'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4efdca753637830f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "> Accuracy: 0.7984 \\\n",
    "> Precision: 0.35526315789473684 \\\n",
    "> Recall: 0.25961538461538464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dac11876dbeaeb49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Great! Now let's obtain our confusion matrix in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86d92e523ba4389c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_sklearn(predictions, target):\n",
    "    \"\"\"\n",
    "    Given the predicted and true labels, return the confusion\n",
    "    matrix using scikitlearn\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series or np.array): classifier predictions     \n",
    "        target (pd.Series or np.array): true labels     \n",
    "\n",
    "    Returns: conf_mat\n",
    "        conf_mat: confusion_matrix\n",
    "    \"\"\"    \n",
    "\n",
    "    # importing the correct functions, return scikit's confusion matrix\n",
    "        # NOTE: Even though this is not a good programming practice, just \n",
    "    #       for the sake of the exercise, ensure you import the right \n",
    "    #       function within this code:\n",
    "    \n",
    "    # Hint:\n",
    "\n",
    "    # from ? import ...\n",
    "    # conf_mat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return conf_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4e5021ddb84188d6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_1 = df.iloc[100:200].copy()\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    confusion_matrix_sklearn(df_1['prediction'], df_1['target']), \n",
    "    [[69, 13], [13,  5]], \n",
    "    2\n",
    ")\n",
    "\n",
    "df_2 = df.iloc[0:50].copy()\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    confusion_matrix_sklearn(df_2['prediction'], df_2['target']), \n",
    "    [[35, 3], [9, 3]], \n",
    "    2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f560d037ce9b5f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And we can check the matrix for the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe94dbd2ee56d159",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Confusion matrix:\\n {confusion_matrix_sklearn(df['prediction'], df['target'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "> Confusion matrix: \\\n",
    "> \\[ \\[ 944  98 \\] \\\n",
    ">   \\[ 154  54 \\] \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a1d01de0d2450f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_roc_sklearn(probas, target):\n",
    "    \"\"\"\n",
    "    Given the predicted probabilities (probas) and true labels, \n",
    "    return the information regarding the roc curve - false positives rate,\n",
    "    true positives rate and thresholds - and the area under the curve\n",
    "    \n",
    "    Args:\n",
    "        probas (pd.Series or np.array): classifier probabilities     \n",
    "        target (pd.Series or np.array): true labels     \n",
    "\n",
    "    Returns: fpr, tpr, thresholds, roc_auc\n",
    "        fpr: false positives rates array for roc curve\n",
    "        tpr: true positives rates array for roc curve\n",
    "        thresholds: thresholds array for roc curve\n",
    "        roc_auc: roc area under the curve\n",
    "\n",
    "    \"\"\"    \n",
    "\n",
    "    # from ? import ... \n",
    "    # fpr, tpr, thresholds = ...\n",
    "    # roc_auc = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return fpr, tpr, thresholds, roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-74e565ebc3dd0dc9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_1 = df.iloc[100:200].copy()\n",
    "\n",
    "fpr_1, tpr_1, thresholds_1, roc_auc_1 = get_roc_sklearn(df_1['probas'], df_1['target'])\n",
    "\n",
    "np.testing.assert_array_almost_equal(fpr_1[3:6], [0.0244, 0.0366, 0.0366], 3)\n",
    "np.testing.assert_array_almost_equal(tpr_1[6:8], [0.1667, 0.2222], 3)\n",
    "np.testing.assert_array_almost_equal(thresholds_1[7:10], [0.3578, 0.3378, 0.3356], 3)\n",
    "np.testing.assert_almost_equal(roc_auc_1, 0.6714, 2)\n",
    "\n",
    "df_2 = df.iloc[0:50].copy()\n",
    "\n",
    "fpr_2, tpr_2, thresholds_2, roc_auc_2 = get_roc_sklearn(df_2['probas'], df_2['target'])\n",
    "\n",
    "np.testing.assert_array_almost_equal(fpr_2[3:6], [0.0526, 0.0526, 0.0789], 3)\n",
    "np.testing.assert_array_almost_equal(tpr_2[1:3], [0.0833, 0.1667], 3)\n",
    "np.testing.assert_array_almost_equal(thresholds_2[7:10], [0.2808, 0.2550, 0.2449], 3)\n",
    "np.testing.assert_almost_equal(roc_auc_2, 0.725, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a976651c2a94bc79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, let's run it for the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f9752ffe16d8a1b6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "fpr_test, tpr_test, thresholds_test, roc_auc_test = get_roc_sklearn(df['probas'], df['target'])\n",
    "print(fpr_test[20:30])\n",
    "print(tpr_test[20:30])\n",
    "print(thresholds_test[30:45])\n",
    "print(roc_auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b38ffdbde384e011",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "    [0.02399232 0.02399232 0.02495202 0.02495202 0.02783109 0.02783109\n",
    "     0.02975048 0.02975048 0.03358925 0.03358925]\n",
    "    [0.10096154 0.10576923 0.10576923 0.11538462 0.11538462 0.12019231\n",
    "     0.12019231 0.12980769 0.12980769 0.13461538]\n",
    "    [0.39655128 0.39446442 0.39220204 0.3893066  0.38817124 0.38330687\n",
    "     0.38128277 0.37884849 0.37641972 0.37601994 0.37019737 0.36928763\n",
    "     0.36803879 0.36659233 0.35809179]\n",
    "    0.6980335523401742"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a54695f047638506",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Congratulations! You made it to the end of another unit! You are now ready to explore more about classification and regression problems and other particularities of model training and selection!\n",
    "\n",
    "See you on the next unit!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
