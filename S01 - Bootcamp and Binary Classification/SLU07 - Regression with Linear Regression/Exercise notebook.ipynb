{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79f2337e7779945a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SLU07 - Regression With Linear Regression: Exercise notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62ddf765d4352694",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the learning notebook we have presented you with several new concepts. \n",
    "\n",
    "With this exercise notebook we expect you to take a closer look to some of the formulas we introduced and also expect you to understand the concepts from a practical experience.\n",
    "\n",
    "Let's dive right into it and enjoy the ride!\n",
    "\n",
    "![ride](assets/ride.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cb0eb9c3a32286f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Base imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-74ff3f984bb74106",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1 - Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1eba3d354cdb022",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As a first exercise, let's imagine you're thinking about changing your career and now you want to become a full-time blogger (I know that in these days, bloggers are not a thing but let's assume it is).\n",
    "\n",
    "\n",
    "![blog](assets/blog.gif)\n",
    "\n",
    "Although being a full-time blogger seems to be a lot of fun, you need to have a sense of how much money you will make with the blog so you can manage your financial life. \n",
    "\n",
    "You do some online research and then you find income reports that full-time bloggers have published and you put that information into a spreadsheet. This spreadsheet has information about the number of months of experience and the amount of money earned.\n",
    "\n",
    "You will use this dataset to predict approximately how much money can be earned based on the number of months of experience blogging.\n",
    "\n",
    "You probably already have a hunch that the older a blog is, the more money it makes, but by using linear regression you’ll be able to support or refute this hypothesis with actual data. You know, the kind of data you can take to the bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c8fc43cd0410a8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_blog = pd.read_csv('data/blogging_income.csv')\n",
    "df_blog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32f343233539ec16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's start by visualizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5687e6f07d6725ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_blog = df_blog.sort_values('MonthsExperience')\n",
    "plt.xlim((2, 20))\n",
    "plt.ylim((200, 1800))\n",
    "plt.xlabel('MonthsExperience')\n",
    "plt.title('Income based on Months of Experience')\n",
    "plt.ylabel('Income')\n",
    "plt.plot(df_blog['MonthsExperience'], df_blog['Income'], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67395307d5b7356e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.1 - Simple Linear Model\n",
    "\n",
    "As you can see, our data has only one variable ($x$: 'Months of Experience') and one label ($y$: 'Income'), so we can try to fit it with a simple linear regression. This model is represented by the following expression:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "where $\\hat{y}$ are the predictions, $\\beta_0$ is the intercept, $\\beta_1$ is the coefficient and $x$ is the input sample. Expanding to several samples, we can write this equation in a vector form:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\beta_0\\vec{1} + \\beta_1 \\vec{x}$$\n",
    "\n",
    "Implement the function <em>simple_linear_regression</em>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0658efc6a6964c6e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_linear_regression(x, betas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples,) - The input data \n",
    "        betas: numpy.array with shape (2,) - The weights of the model [b_0, b_1]\n",
    "    \n",
    "    Returns:\n",
    "        f1, f2 : numpy.array with shape (num_samples,) - intermediate calculations\n",
    "        y_hat : list (lenght = (num_samples)) - The prediction made by \n",
    "                the simple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's begin with the first term of the equation\n",
    "    # f1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, the second term\n",
    "    # f2 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now let's put all together\n",
    "    # y_hat = ...\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return f1, f2, y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-520e93f68c350db0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check if your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-651ae1ad52511300",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple test\n",
    "f1_1, f2_1, y_hat1 = simple_linear_regression(np.arange(0, 5), np.array([-15, 20]))\n",
    "expected_hash_1 = 'ef2d127de37b942baad06145e54b0c619a1f22327b2ebbcfbec78f5564afe39d'\n",
    "expected_hash_2 = 'b63d2b235e273730eda06df31b5d8c0f4c73eec62deb0a3937bc3540384d6a26'\n",
    "expected_hash_3 = '981a391f15355b5d3f9fa774f7e5e2080b001ddf5e358b2b18c1066ce44dbd9e'\n",
    "assert hashlib.sha256(str(len(f1_1)).encode('utf-8')).hexdigest() == expected_hash_1, \"Perhaps your f1 variable is not well calculated!\"\n",
    "assert hashlib.sha256(str(f1_1[-1]).encode('utf-8')).hexdigest() == expected_hash_2, \"Make sure to use the beta_0 and the right dimension of x \"\n",
    "assert hashlib.sha256(str(f1_1[0]).encode('utf-8')).hexdigest() == expected_hash_2, \"Make sure to use the beta_0 and the right dimension of x\"\n",
    "assert hashlib.sha256(str(len(f2_1)).encode('utf-8')).hexdigest() == expected_hash_1, \"Perhaps your f2 variable is not well calculated!\"\n",
    "assert hashlib.sha256(str(type(y_hat1)).encode('utf-8')).hexdigest() == expected_hash_3, \"Pay attention to the type of the output\"\n",
    "np.testing.assert_array_almost_equal(y_hat1, np.array([-15.0, 5.0, 25.0, 45.0, 65.0]))\n",
    "\n",
    "# Test using our dataset\n",
    "f1_2, f2_2, y_hat2 = simple_linear_regression(df_blog['MonthsExperience'],np.array([3.0, 2.5]))\n",
    "expected_hash_4 = '624b60c58c9d8bfb6ff1886c2fd605d2adeb6ea4da576068201b6c6958ce93f4'\n",
    "expected_hash_5 = 'a416ea84421fa7e1351582da48235bac88380a337ec5cb5a9239dc7d57908b4b'\n",
    "expected_hash_6 = '981a391f15355b5d3f9fa774f7e5e2080b001ddf5e358b2b18c1066ce44dbd9e'\n",
    "assert hashlib.sha256(str(len(f1_2)).encode('utf-8')).hexdigest() == expected_hash_4, \"Perhaps your f1 variable is not well calculated!\"\n",
    "assert hashlib.sha256(str(f1_2[-1]).encode('utf-8')).hexdigest() == expected_hash_5, \"Make sure to use the beta_0 and the right dimension of x \"\n",
    "assert hashlib.sha256(str(f1_2[0]).encode('utf-8')).hexdigest() == expected_hash_5, \"Make sure to use the beta_0 and the right dimension of x\"\n",
    "assert hashlib.sha256(str(len(f2_2)).encode('utf-8')).hexdigest() == expected_hash_4, \"Perhaps your f2 variable is not well calculated!\"\n",
    "assert hashlib.sha256(str(type(y_hat2)).encode('utf-8')).hexdigest() == expected_hash_6, \"Pay attention to the type of the output\"\n",
    "np.testing.assert_array_almost_equal(np.array(y_hat2)[[2,8,11,15,20, 23]].tolist(), \n",
    "   [10.5, 20.5, 25.5, 30.5, 35.5, 40.5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b2f4bcb7e195509c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ok, so know that you have a function to construct the model, the next step is to discover the values of betas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc935eebba6de6f9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The first approach you can take is to implement the closed form solution. This is, solving the equation that minimizes the error across all of the samples - ordinary least squares. For that, however, we need to understand what the error trying to be minimised is. Let's take a look at the cost function you have learned:\n",
    "\n",
    "### Exercise 1.2 Cost Function\n",
    "\n",
    "Start by implementing the cost function presented - mean squared error:\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N e_i^2 = \\frac{1}{N} \\sum_{n=1}^N (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "where the error is the difference between your predictions and the actual sample value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1049431fa099d4b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y : numpy.array with shape (num_samples, ) - real target\n",
    "        y_hat : numpy.array  with shape (num_samples, ) - predicted target\n",
    "    \n",
    "    Returns:\n",
    "        mean_squared_error : float\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the error\n",
    "    # error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, square the difference\n",
    "    # squared_error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Finally, take the mean and return the error\n",
    "    # mean_squared_error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f2a4da7fe193783",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4510d1dc5df769f3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple tests\n",
    "np.testing.assert_almost_equal(cost_function(np.array([.24]), np.array([.36])), 0.0144)\n",
    "np.testing.assert_almost_equal(cost_function(np.array([1.13]), np.array([2.65])), 2.3104)\n",
    "\n",
    "# Test using our dataset\n",
    "x_rnd = df_blog['MonthsExperience'].values\n",
    "y_rnd = df_blog['Income'].values\n",
    "beta_rnd = np.array([120, 120])\n",
    "_, _,y_hat_rnd = simple_linear_regression(x_rnd, beta_rnd)\n",
    "\n",
    "np.testing.assert_almost_equal(cost_function(y_rnd, y_hat_rnd), 294785.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab4aadf231356588",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see from the previous test, picking just random values for our weights will probably yield very high error values. You can even visualize this to see that in fact these random weights don't fit our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3963d719eccbc48",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((2, 20))\n",
    "plt.ylim((200, 2500))\n",
    "plt.xlabel('MonthsExperience')\n",
    "plt.ylabel('Income')\n",
    "plt.title('Prediction with random weights')\n",
    "plt.plot(df_blog['MonthsExperience'], df_blog['Income'], 'b.', label='True')\n",
    "plt.plot(df_blog['MonthsExperience'], y_hat_rnd, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b7d6627f93ad176",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.3 - Closed Form Solution\n",
    "\n",
    "Lets then implement a closed form solution in order to get the optimal values for the betas. Remember the solution to minimize the error can be written as:\n",
    "\n",
    "$$ \\beta_1 = \\frac{\\sum_{i}^{N}{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i}^{N}{(x_i - \\bar{x})^2}} = \\frac{cov(x, y)}{var(x)}$$\n",
    "\n",
    "with cov(x,y) and var(x) are, respectively, the covariance and variance of the samples\n",
    "\n",
    "$$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$ \n",
    "\n",
    "where $\\bar{y} = \\frac{1}{N}\\sum_{i}^{N}{y_i}$ and $\\bar{x} = \\frac{1}{N}\\sum_{i}^{N}{x_i}$ are the means of the sample.\n",
    "\n",
    "Complete the closed form solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c6179999833a9a8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def closed_form_solution(x, y):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x : numpy.array with shape (num_samples, ) - input samples \n",
    "        y : numpy.array with shape (num_samples, ) - sample labels\n",
    "    \n",
    "    Returns:\n",
    "        b0: float\n",
    "        b1: float\n",
    "    \"\"\"\n",
    "    # The sample covariance and variance for 1-d arrays in \n",
    "    # numpy for this particular case are computed as follows\n",
    "    # We covered this part so you don't lose too much time on these details\n",
    "    cov_xy = np.cov(x, y, bias=True)[0][1]\n",
    "    var_x = np.var(x)\n",
    "    \n",
    "    # Compute coefficient beta_1\n",
    "    # b1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute intersect beta_0\n",
    "    # b0 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return b0, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea54ae86dbd717c8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ed0cf5f5ace73f0a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Simple test\n",
    "np.testing.assert_array_almost_equal(\n",
    "    closed_form_solution(np.arange(0, 10), np.arange(0, 20, 2)),\n",
    "    (0.0, 2.0)\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    closed_form_solution(np.arange(-2, 3), np.array([-1.5, -.56, .26, 1.3, 2.5])),\n",
    "    np.array([.4, .986])\n",
    ")\n",
    "\n",
    "#Test using our dataset\n",
    "x_cf = df_blog['MonthsExperience'].values\n",
    "y_cf = df_blog['Income'].values\n",
    "beta_cf = closed_form_solution(x_cf, y_cf)\n",
    "y_hat_cf = simple_linear_regression(x_cf, beta_cf)[2]\n",
    "\n",
    "np.testing.assert_array_almost_equal(cost_function(y_cf, y_hat_cf), 10170.468711301624)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f64b1ed948d0959c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can also visualize how good your solution fits the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c294660043fd604",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((2, 20))\n",
    "plt.ylim((200, 2000))\n",
    "plt.xlabel('MonthsExperience')\n",
    "plt.ylabel('Income')\n",
    "plt.title('Predicion with best pred with closed form solution')\n",
    "plt.plot(df_blog['MonthsExperience'], df_blog['Income'], 'b.', label='True')\n",
    "plt.plot(df_blog['MonthsExperience'], y_hat_cf, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-87eed9f8fb8bc294",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.4 - Final Answer\n",
    "\n",
    "Okay... Let’s say you decide you’re willing to give blogging 6 months of your time. After that point, if you’re not making any money you’ll call it quits. So, giving the model and the functions you already constructed, let's see how much money you can make with 6 months of blogging:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-63a7d5b792053937",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can use the same code of above to create the variables we need\n",
    "x_simple = df_blog['MonthsExperience'].values\n",
    "y_simple = df_blog['Income'].values\n",
    "\n",
    "#Let's calculate the best coefficients\n",
    "beta_simple = closed_form_solution(x_simple, y_simple)\n",
    "\n",
    "# Now, let's calculate the predicted salary using one of the above functions we already implemented above\n",
    "# Hint: If you don't remember the types/shapes of input/output of the function, it will be a good idea to check.\n",
    "# y_hat_simple = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('After 6 months in blogging, you will make: {} €'.format(round(y_hat_simple[0], 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0e90b401208965c6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(y_hat_simple, list)\n",
    "np.testing.assert_array_almost_equal(cost_function(y_simple, y_hat_simple), 208116.67604822144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28ca0978a921d6b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "![mission](assets/rich.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb49a1b1ed811039",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6c1ed785a80aad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So, maybe blogging isn't a very profitable job in a short/medium-term, at least...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-791ecdf680235b04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-708eb8c290452c85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this section, we will expand what we learned to a linear regression with multiple inputs - which we call features of our model. We will use a very specific scenario so we are able to visualize it better - we will try to model a polynomial function, in particular, a cubic function, which can be written as:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^{2} + \\beta_3 x^{3}$$\n",
    "\n",
    "You will basically be considering each power of x as a different feature. To simplify, we are going to construct a dataset with the powers we want for this, so let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-349d6ec88f5f307d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "# transforming the data to include another axis\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "polynomial_features= PolynomialFeatures(degree=3)\n",
    "x_poly = polynomial_features.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cee8fdcad5e51a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data_ml = pd.DataFrame(x_poly, columns=['x1', 'x2', 'x3', 'x4']).drop(['x1'], axis=1)\n",
    "data_ml.columns = ['x0', 'x1', 'x2']\n",
    "data_ml['y'] = pd.DataFrame(y)[0]\n",
    "\n",
    "\n",
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e2ef3862e8aae7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_ml = data_ml.sort_values('x0')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Relation between x0 and y')\n",
    "plt.plot(data_ml['x0'], data_ml['y'], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9f9d3c78f620b9f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Although this model is non linear in its features, notice that it is linear with respect to the weights, and the equation above can be rewritten as\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_0 + \\beta_2 x_1 + \\beta_3 x_2$$\n",
    "\n",
    "where $[x_0, x_1, x_2]$ is our feature vector for a given sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79fedc69b50dbe6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 2.1  Linear Model Extended\n",
    "\n",
    "The multiple linear regression problem is just the linear regression problem on a linear model with several inputs. This model can be represented by the following expressions:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\sum_{i=1}^K \\beta_i x_i$$\n",
    "\n",
    "We can also write it in matrix form to consider several samples, as before:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\beta_0\\vec{1} + \\vec{\\beta_{1-k}}X^T$$\n",
    "\n",
    "where X is now a matrix, containing all features for all samples: \n",
    "\n",
    "$$ X = \\begin{bmatrix} \n",
    "x_1^1 & x_1^2 & ... & x_1^k \\\\\n",
    "x_2^1 & x_2^2 & ... & x_2^k \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "x_n^1 & x_n^2 & ... & x_n^k \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "If you go back to the closed form solution you implemented before, you might notice that we had already used matrix form, in particular to concatenate our whole weight vector. We'll follow the same logic, and extend our matrix X to allow a column of ones:\n",
    "\n",
    "$$ X' = [\\vec{1} | X] $$\n",
    "\n",
    "and rewrite:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\vec{\\beta}(X')^T$$\n",
    "\n",
    "Implement below this extended model.\n",
    "\n",
    "Tip: You might want to review the learning notebook and examples to get used to the matrix handling in the following problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10eb3e761476d6d9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extended_linear_model(x, betas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        betas : numpy.array with shape (num_features + 1,) - weights of \n",
    "                our model, with the intercept in the first position of \n",
    "                the array\n",
    "    \n",
    "    Returns:\n",
    "        y_pred : list(len= (num_samples)) - prediction \n",
    "                made by the simple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We do the proper reshaping of weights so you don't have \n",
    "    # to worry about that and focus on the remaining logic\n",
    "    betas = betas.reshape((1, -1))\n",
    "    \n",
    "    # Extend the matrix x with a column of ones\n",
    "    # X_mat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Compute the output of the linear model\n",
    "    # y_pred = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Once again, we reshape your array to get the proper output\n",
    "    return y_pred.flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a1c4e3d2ce2183d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Since this is an extension of the simple linear model, it should be able to cover that use case also. Check that your solution still passes the test for the simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d853a799cc5992d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple test\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(0, 10).reshape(-1, 1), np.array([-12, 30])), \n",
    "    [-12.,  18.,  48.,  78., 108., 138., 168., 198., 228., 258.])\n",
    "\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(-5, 5).reshape(-1, 1), np.array([1, 1])), \n",
    "    [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.]\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(-10, 10, 2).reshape(-1, 1), np.array([0.25, 2.1])), \n",
    "    [-20.75, -16.55, -12.35, -8.15, -3.95, 0.25, 4.45, 8.65, 12.85, 17.05]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a8e58f5e96ecf1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now check your solution passes the tests for the extended version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8c9cab5fa1ebfc99",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.array([[1., 2.], [3., 4.], [5., 6.]]), np.array([-1., 0., 1.])), \n",
    "    [1., 3., 5.]\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.ones((10, 2)), np.array([1., 2., 3.])), \n",
    "    [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae733317d24d320d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Summed Squared Error\n",
    "\n",
    "As before, we can use the summed squared error as our cost. However, since this function does not receive anything other than the predictions and true values, there is no need to reimplement it. As before, let's see how the model would behave and what would be its error if we pick random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ebb1e0761f537021",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Error in our dataset with random weights\n",
    "x_pln_rnd = data_ml['x0']\n",
    "X_pln_rnd = data_ml.drop('y', axis=1).to_numpy()\n",
    "y_pln_rnd = data_ml['y'].values\n",
    "beta_pln_rnd = np.array([1., 1., -.5, 0.])\n",
    "y_hat_pln_rnd = extended_linear_model(X_pln_rnd, beta_pln_rnd)\n",
    "\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Multiple Linear Regression')\n",
    "plt.plot(x_pln_rnd, y_pln_rnd, 'b.', label='true')\n",
    "plt.plot(x_pln_rnd, y_hat_pln_rnd, 'r-', label='pred')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Error: {}'.format(cost_function(y_pln_rnd, y_hat_pln_rnd))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e3366d377d3c9b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see, the solution is clearly not a fit, and the error is very high. So let's move into our closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ff21e3c52fafce2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 2.2 - Closed Form Solution\n",
    "\n",
    "Let's now implement the closed form solution for the generic case. When put into matrix form, remember the solution to minimize the error can be written as:\n",
    "\n",
    "$$ \\vec{\\beta} = (X^TX)^{-1}(X^T\\vec{y})$$\n",
    "\n",
    "\n",
    "Where X is our matrix of samples extended to add a 1 component in each sample, $X = [\\vec{1} | X] $ , $\\vec{y}$ is the output vector, and $\\vec{\\beta}$ the weight vector with weights $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "Implement the closed form solution for the multiple linear regression problem below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e87f47eb6ec320ee",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extended_closed_form_solution(x, y):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples, ) - sample labels\n",
    "    \n",
    "    Returns:\n",
    "        betas : list (len= (num_features + 1)) - weight vector \n",
    "    \"\"\" \n",
    "    \n",
    "    # Proper reshaping of the labels\n",
    "    y = y.reshape((-1, 1))\n",
    "\n",
    "    # Extend vector of samples with array of ones\n",
    "    # X_extended = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute betas\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Once again, we reshape your array to get the proper output\n",
    "    return betas.flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ec27afe2a6ee23b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-47587567e05c2602",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Old tests\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.arange(0, 10).reshape(10, 1), np.arange(0, 20, 2).reshape(10, 1)),\n",
    "    [0., 2.]\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.arange(-2, 3).reshape(5, 1), np.array([-1.25, -.5, .25, 1., 1.75]).reshape(5, 1)),\n",
    "    [.25, .75]\n",
    ")\n",
    "\n",
    "\n",
    "# Extended test cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.array([[1., -1.], [2., 1.], [3., -5.]]), np.array([0., 1., 0.])), \n",
    "    [-0.25, 0.5, 0.25]\n",
    ")\n",
    "\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.array([[10., -2.], [-4., 5.], [-7., -8.]]), np.array([2., 1., -.5])), \n",
    "    np.array([1.019704, 0.115764, 0.08867])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbdc5cd776150269",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now apply it to our dataset to get the best weights, and measure the error across the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fd38814320080448",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_pln_cf = data_ml['x0'].values\n",
    "y_pln_cf = data_ml['y'].values\n",
    "\n",
    "# All columns except y\n",
    "X_pln_cf = data_ml.drop('y', axis=1).to_numpy()\n",
    "\n",
    "beta_pln_cf = extended_closed_form_solution(X_pln_cf, y_pln_cf)\n",
    "y_hat_pln_cf = extended_linear_model(X_pln_cf, np.array(beta_pln_cf))\n",
    "\n",
    "assert math.isclose(cost_function(y_pln_cf, y_hat_pln_cf), 7.159451499759534), \"Check your closed form function!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d204ff04dacb5dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "And finally we'll try to see how well this solution fits the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33d9aab23eb2dadd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predicion with best pred with closed form solution for multiple linear regression')\n",
    "plt.plot(x_pln_cf, y_pln_cf, 'b.', label='True')\n",
    "plt.plot(x_pln_cf, y_hat_pln_cf, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ecb84f49c6e98b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Much better! You have a much better fit.\n",
    "\n",
    "![reaction](assets/reaction.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4bbf62d226129f9a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 3 . ScikitLearn Linear Regression\n",
    "\n",
    "Luckily, ScikitLearn already provides us with a solver for the Linear Regression problem, which implements a closed form solution internally. It also provides already some extra info on the regression, such as the $R^2$ score. \n",
    " \n",
    "For this exercise we'll be using the Boston housing dataset, and try to model the house pricing through the provided features. Start by loading and looking into the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cc093acaeccc281",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/boston (scaled).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cc95feb70c42b3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# All features except house price\n",
    "columns_housing = data.drop('MEDV', axis=1)\n",
    "x_housing = columns_housing.to_numpy()\n",
    "\n",
    "# House price \n",
    "y_housing = data['MEDV'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbd3a2e4e2107803",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Each of the columns in the table is one of the features our model is going to use, this is, one of the inputs we are going to give it. Use the `sklearn.linear_model.LinearRegression` module that you've learned and implement it in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4896df37a4f5670c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def sklearn_model_regression(x, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args: \n",
    "        x: numpy.ndarray with shape (num_samples, num_features) - samples of our model\n",
    "        y: numpy.array with shape (num_samples, ) - sample labels\n",
    "        \n",
    "    Return:\n",
    "        coefs: list (len= (num_features) - coefficients vector\n",
    "        intercept: float - intercept value\n",
    "        score: float - R squared score of regression\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the linear regressor\n",
    "    # lr = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    # coefs = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the intercept\n",
    "    # intercept = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Calculate the score\n",
    "    # score = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return coefs, intercept, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25a710d3ea831ce0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's see then what our coefficients are for each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-edea816492fd3f98",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "coefs_housing, intercept_housing, score_housing = sklearn_model_regression(x_housing, y_housing)\n",
    "\n",
    "print('Feature coefficients: ')\n",
    "print(pd.Series(coefs_housing, columns_housing.columns))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept: {}'.format(intercept_housing))\n",
    "print('\\n')\n",
    "\n",
    "print('R² score: {}'.format(score_housing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1aca587618ebd00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dfb0a7be54a211de",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "betas_housing = np.concatenate((np.array([intercept_housing]), np.array(coefs_housing)), axis=0)\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(x_housing[:10], betas_housing),\n",
    "    np.array([30.00821269, 25.0298606, 30.5702317, 28.60814055, 27.94288232, \n",
    "              25.25940048, 23.00433994, 19.5347558, 11.51696539, 18.91981483])\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(x_housing[-10:], betas_housing),\n",
    "    np.array([14.01017244, 19.10825534, 21.29720741, 18.45524217, 20.46764235, \n",
    "              23.53261729, 22.37869798, 27.62934247, 26.12983844, 22.34870269])\n",
    ")\n",
    "\n",
    "y_hat_housing = extended_linear_model(x_housing, betas_housing)\n",
    "assert math.isclose(cost_function(y_housing, y_hat_housing), 21.8977792176875)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d32d00c1a044a960",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We can also use this model to get to the solution for our previous problems. Run the cells below and see the scikitlearn solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8b2e2340ecbae20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Your co-worker was giving you some hints to help you in this last task. She is very nice and helpful and she suggests that you try a more generic method - the Gradient Descent! You've heard of it so although you are very tired already you give it a chance! \n",
    "\n",
    "Last one before you go home!\n",
    "\n",
    "\n",
    "![tired](assets/tired.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d547f4157ab02c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4. Gradient Descent\n",
    "\n",
    "Now we will see how to get to a similar solution through learning methods. In this section, you will implement gradient descent. This method is an iterative process that updates the weights in the direction that minimizes our error. For this it makes use of derivatives. The formula follows:\n",
    "\n",
    "$$ \\vec{\\beta}_{i+1} = \\vec{\\beta}_i - \\eta \\Delta_\\vec{\\beta} J$$\n",
    "\n",
    "where $\\Delta_\\vec{\\beta}$ is a vector of the derivatives - also called gradients - of our error function with respect to the weights, $\\beta_{i+1}$ is the updated weight and $\\beta_{i}$ the current weight. We then need to be able to compute these gradients to be able to update the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54fd3a31f51054",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Multiple Linear Regression partial derivatives\n",
    "\n",
    "The vector $\\Delta_\\vec{\\beta}$ in the formula above is just a vector with the partial derivatives of the error function with respect to each of the weights. The formulas for these partial derivatives with respect to each weight are defined as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_0} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) $$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_1} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n)x_{1_n} $$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_K} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n)x_{K_n} $$\n",
    "\n",
    "Since the focus of this notebook is for you to implement the methods to solve linear regression, and you already have quite some work, we'll solve this one for you. Check below the code for the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-315f5fa32e6031cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_partial_derivatives(x, y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples,) - sample labels\n",
    "        y_hat : numpy.array with shape (num_samples,) - predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        deltas : pandas.Series shape (num_features + 1,)\n",
    "            \n",
    "    \"\"\"    \n",
    "\n",
    "    # Compute the difference between the targets and the predictions.\n",
    "    y_diff = y - y_hat\n",
    "    \n",
    "    # Initialize the numpy array of partial derivatives\n",
    "    deltas = np.zeros((x.shape[1] + 1, ))\n",
    "    \n",
    "    # Compute the partial derivative for b0\n",
    "    deltas[0] = -(2 * y_diff).mean()\n",
    "    \n",
    "    # Extract the partial derivatives of the remaining betas  \n",
    "    for col in range(x.shape[1]): \n",
    "        deltas[col+1] = -((2 * y_diff) * x[:, col]).mean()\n",
    "    \n",
    "    \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-664e5ef6f8b4ed2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 4.1 Adjusting  parameters with gradient descent\n",
    "\n",
    "Now we want to adjust the weights with the update rule we presented:\n",
    "\n",
    "$$ \\vec{\\beta}_{i+1} = \\vec{\\beta}_i - \\eta \\Delta_\\vec{\\beta} J$$\n",
    "\n",
    "where $\\eta$ is our learning rate - how fast we want to move in the direction of the gradient. We will be implementing the standard gradient descent, also know as batch gradient descent, where for each iteration we will compute the derivatives by taking in all the dataset:\n",
    "\n",
    "1. _For epoch in 1...epochs:\n",
    "    1. Predict the outputs with current weights $\\hat{y} = \\vec{\\beta}_i X$\n",
    "    2. $\\Delta_{\\beta_0} = \\frac{1}{N} \\sum_{n=1}^N 2 (y - \\hat{y})$\n",
    "    3. $\\Delta_{\\beta_{i=1...N}} = \\frac{1}{N} \\sum_{n=1}^N 2 (y - \\hat{y})x_{i_n} $\n",
    "    4. $\\beta_i = \\beta_i - \\eta \\Delta_{\\beta_i}$\n",
    "\n",
    "Notice that you can get the gradients in steps A, B and C with the function implemented above. \n",
    "\n",
    "The number of epochs and learning rate will impact how fast and how good the solution we converge to. Besides the number of epochs there are more clever ways of knowing when to stop this procedure, but for simplicity, we will only use this one here.\n",
    "\n",
    "Implement this gradient descent function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f64edb60cc15b1a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient_descent(x, y, betas, learning_rate, epochs): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples,)  - sample labels\n",
    "        betas : numpy.array with shape (num_features + 1,) - initial weights\n",
    "        learning_rate : float - factor that will define the size of update step\n",
    "        epochs : int - number of times to run full dataset\n",
    "\n",
    "    Returns:\n",
    "        betas : list (len= (num_features + 1)) - final weights after algorithm\n",
    "            \n",
    "    \"\"\"    \n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        # Compute estimates for this iteration \n",
    "        # y_hat = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        y_hat = np.array(y_hat)\n",
    "\n",
    "        # Compute the partial derivatives of the error function \n",
    "        # (hint: check linear_regression_partial_derivatives)\n",
    "        # deltas = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Update betas with Gradient Descent rule \n",
    "        # betas = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return betas.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9251131ceba84a3f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b26ec5b38f334783",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    betas_, \n",
    "    np.array([\n",
    "        20.1536, -0.391 ,  0.7281, -0.1594,  0.9096, -0.4351,  3.3176,\n",
    "        0.2957, -0.9711,  0.5549, -0.8421, -1.4982,  1.0387, -3.0292]), \n",
    "    decimal=4)\n",
    "\n",
    "\n",
    "np.random.seed(84)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 1\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    betas_, \n",
    "    np.array([ 4.5434, -0.4652,  0.9789, -0.4454,  1.1187, -0.7988,  1.5215,\n",
    "       -0.581 ,  0.9902, -0.7336, -0.745 , -0.5769,  1.5129, -1.1906]), \n",
    "    decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84a4265ff22c40c5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In order to compare the coefficients between the closed form solution and the result from sklearn model, run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3aea3af7aafc4f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Read Dataset\n",
    "data = pd.read_csv('data/boston (scaled).csv')\n",
    "\n",
    "columns_housing = data.drop('MEDV', axis=1)\n",
    "x_housing = columns_housing.to_numpy()\n",
    "y_housing = data['MEDV'].to_numpy()\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "coefs_housing, intercept_housing, score_housing = sklearn_model_regression(x_housing, y_housing)\n",
    "\n",
    "intercept_housing_sgd = betas_[0]\n",
    "coefs_housing_sgd = betas_[1:]\n",
    "\n",
    "series_sgd = pd.Series(coefs_housing_sgd, columns_housing.columns, name='SGD')\n",
    "series_ols = pd.Series(coefs_housing, columns_housing.columns, name='OLS')\n",
    "\n",
    "print('Feature coefficients: ')\n",
    "print(pd.concat([series_sgd, series_ols], axis=1))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept SGD: {}'.format(intercept_housing_sgd))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept OLS: {}'.format(intercept_housing))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a4e45d100fa0d313",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This is it! The end of your first day and the end of this learning unit! \n",
    "\n",
    "\n",
    "![sum](assets/sum.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
