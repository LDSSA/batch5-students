{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus notebook - derivatives and closed form for multiple linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1) Error function:\n",
    "\n",
    "Remember our error function definition, from the learning notebook, which is the same as in the simple model: \n",
    "\n",
    "$$J(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $$ \n",
    "\n",
    "Expanding with our linear model, we get:\n",
    "\n",
    "$$J(y, \\hat{y}) = \\sum_{i=1}^N (y_i - \\beta_0 -  \\sum_{j=1}^K \\beta_k x_{k_i})^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Derivatives of error function for simple linear regression\n",
    "\n",
    "The derivatives are not required for the closed form solution, however they are quite usefull for other methods, such as the gradient descent, which you learn at the end of the learning notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 ) Intercept derivative:\n",
    "\n",
    "The intercept derivative is the same. Let's develop it from our equation above:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{d}{d \\beta_0} (\\frac{1}{N}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_k x_{k_i})^2) $$\n",
    "\n",
    "We can expand the square, without unrolling the sum:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{d}{d \\beta_0} (\\frac{1}{N}\\sum_{i=1}^N (y_i^2 - 2 y_i \\beta_0 - 2 y_i \\sum_{j=1}^K \\beta_k x_{k_i} + 2 \\beta_0\\sum_{j=1}^K \\beta_k x_{k_i}+ \\beta_0^2  + (\\sum_{j=1}^K \\beta_k x_{k_i})^2 )) $$\n",
    "\n",
    "Which makes it easier to cut all the terms that do not depend on $\\beta_0$, since the whole term $\\sum_{j=1}^K \\beta_k x_{k_i}$ is completely independent of $\\beta_0$\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N (0 - 2 y_i - 0 + 2 \\sum_{j=1}^K \\beta_k x_{k_i} + 0 + 2\\beta_0)  \\\\\n",
    "\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N (-2 y_i + 2 \\sum_{j=1}^K \\beta_k x_{k_i} + 2\\beta_0) $$\n",
    "\n",
    "Finally, we'll rearange the interior of the sum and get to:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = -\\frac{1}{N} \\sum_{i=1}^N [2 (y_i - \\sum_{j=1}^K \\beta_k x_{k_i} - \\beta_0)] $$\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = -\\frac{1}{N} \\sum_{i=1}^N [2 (y_i - \\hat{y_i})] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 ) Coefficient derivative:\n",
    "\n",
    "We'll now derivate with respect to each $\\beta_k$. This might seem trickier, since $\\sum_{j=1}^K \\beta_k x_{k_i})$ can not be considered independent of this term, however you'll see that this will get quite simplified due to only one term of the sum being import for each $\\beta_k$. Assume from here on that $k \\in [1, ..., K]$ where K is the number of features of the model. Let's start from the basic expression:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = \\frac{d}{d \\beta_k} (\\frac{1}{N}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_j x_{j_i})^2) $$\n",
    "\n",
    "We'll compute the square still without unrolling the linear model sum:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = \\frac{d}{d \\beta_k} (\\frac{1}{N}\\sum_{i=1}^N (y_i^2 - 2 y_i \\beta_0 - 2 y_i \\sum_{j=1}^K \\beta_j x_{j_i} + 2 \\beta_0\\sum_{j=1}^K \\beta_j x_{j_i}+ \\beta_0^2  + (\\sum_{j=1}^K \\beta_j x_{j_i})^2 )) $$\n",
    "\n",
    "Now let's cut out the terms that don't matter: \n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = \\frac{d}{d \\beta_k} (\\frac{1}{N}\\sum_{i=1}^N (0 - 0 - 2 y_i \\sum_{j=1}^K \\beta_j x_{j_i} + 2 \\beta_0\\sum_{j=1}^K \\beta_j x_{j_i}+ 0  + (\\sum_{j=1}^K \\beta_j x_{j_i})^2 )) $$\n",
    "\n",
    "And now we'll propagate the derivative inside the sum:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = (\\frac{1}{N}\\sum_{i=1}^N (0 - 0 - 2 y_i \\frac{d}{d \\beta_k}\\sum_{j=1}^K \\beta_j x_{j_i} + 2 \\beta_0\\frac{d}{d \\beta_k}\\sum_{j=1}^K \\beta_j x_{k_i}+ 0  + \\frac{d}{d \\beta_k}(\\sum_{j=1}^K \\beta_j x_{j_i})^2 )) $$\n",
    "$$\\frac{d J}{d \\beta_k} = (\\frac{1}{N}\\sum_{i=1}^N (-2 y_i \\frac{d}{d \\beta_k}\\sum_{j=1}^K \\beta_j x_{j_i} + 2 \\beta_0\\frac{d}{d \\beta_k}\\sum_{j=1}^K \\beta_j x_{j_i}+ \\frac{d}{d \\beta_k}(\\sum_{j=1}^K \\beta_j x_{j_i})^2 )) $$\n",
    "\n",
    "#### Derivative of the sum\n",
    "\n",
    "The first two terms can be computed quite easily, since we know that:\n",
    "\n",
    "$$\\sum_{j=1}^K \\beta_j x_{j_i} =  \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + ... + \\beta_K x_{K_i} $$\n",
    "\n",
    "So the derivative with respect to $\\beta_k$ will concern only one of the terms in the sum, namely $\\beta_k x_{k_i}$:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k}\\sum_{j=1}^K \\beta_j x_{j_i} = x_{k_i}$$\n",
    "\n",
    "\n",
    "#### Derivative of the square of sum\n",
    "\n",
    "To solve the derivative of the square of the sum, since we know we are only interested in terms depending on $\\beta_k$, we can rewrite it as follows:\n",
    "\n",
    "$$(\\sum_{j=1}^K \\beta_j x_{j_i})^2 = (\\beta_k x_{k_i} + \\sum_{j=1, j!=k}^K \\beta_j x_{j_i})^2$$\n",
    "\n",
    "This can be developed to:\n",
    "\n",
    "$$(\\beta_k^2 x_{k_i}^2 + 2 \\beta_k x_{k_i} \\sum_{j=1, j!=k}^K \\beta_j x_{j_i} + (\\sum_{j=1, j!=k}^K \\beta_j x_{j_i})^2$$\n",
    "\n",
    "And so the derivative with respect to $\\beta_k$ becomes:\n",
    "\n",
    "$$ \\frac{d}{d \\beta_k} (\\sum_{j=1}^K \\beta_j x_{j_i})^2 = 2 \\beta_k x_{k_i}^2 + 2 x_{k_i} \\sum_{j=1, j!=k}^K \\beta_j x_{j_i} $$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$$ \\frac{d}{d \\beta_k} (\\sum_{j=1}^K \\beta_j x_{j_i})^2 = x_{k_i} (2 \\beta_k x_{k_i} + 2 \\sum_{j=1, j!=k}^K \\beta_j x_{j_i}) = x_{k_i} (2 \\sum_{j=1}^K \\beta_j x_{j_i})$$\n",
    "\n",
    "\n",
    "#### Putting everything together\n",
    "\n",
    "And finally, our expression becomes: \n",
    "\n",
    "$$\\frac{d}{d \\beta_k} = (\\frac{1}{N}\\sum_{i=1}^N (-2 y_i x_{k_i} + 2 \\beta_0 x_{k_i}+ 2 x_{k_i} \\sum_{j=1}^K \\beta_j x_{j_i}) $$\n",
    "\n",
    "Which we simplify to:\n",
    "\n",
    "$$ \\frac{d J}{d \\beta_1} = -\\frac{1}{N}\\sum_{i=1}^N [2( y_i - \\beta_0 - \\sum_{j=1}^K \\beta_j x_{j_i})x_{k_i}] \\\\\n",
    " \\frac{d J}{d \\beta_1} = -\\frac{1}{N}\\sum_{i=1}^N [2( y_i - \\hat{y_i})x_{k_i}] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Closed form solution for multiple linear regression\n",
    "\n",
    "The multiple linear regression closed form solution actually does not require you to demonstrate those derivatives above, although it still makes use of them. This is mostly because it makes use of the matrix form, which provides some handy rules that simplify the process.\n",
    "\n",
    "First we define our model in matrix notation, where we replace the vector notation from $\\vec{\\beta}$ by $\\boldsymbol{\\beta}$:\n",
    "\n",
    "$$\\hat{y} = X\\boldsymbol{\\beta} $$\n",
    "\n",
    "where X is the matrix of inputs with an additional first column of ones:\n",
    "\n",
    "$$ X' = [\\vec{1} | X] $$\n",
    "\n",
    "Using the same error function, we can write the gradients of the error function as follows:\n",
    "\n",
    "$$ \\Delta_{\\boldsymbol{\\beta}} J  = \\Delta_{\\boldsymbol{\\beta}} (y - X\\boldsymbol{\\beta})^2  $$ \n",
    "\n",
    "First, we'll develop the square, with the following rule in mind: $ A^2 = A^TA$ and that $(AB)^T = B^TA^T$:\n",
    "\n",
    "$$ \\Delta_{\\boldsymbol{\\beta}} J  = \\Delta_{\\boldsymbol{\\beta}} (y - X\\boldsymbol{\\beta})^T(y - X\\boldsymbol{\\beta})  $$ \n",
    "\n",
    "$$ \\Delta_{\\boldsymbol{\\beta}} J  = \\Delta_{\\boldsymbol{\\beta}} (y^T - \\boldsymbol{\\beta}^TX^T)(y - X\\boldsymbol{\\beta})  $$ \n",
    "\n",
    "$$ \\Delta_{\\boldsymbol{\\beta}} J  = \\Delta_{\\boldsymbol{\\beta}} (y^Ty - y^T X \\boldsymbol{\\beta} + \\boldsymbol{\\beta} ^T  X^T X \\boldsymbol{\\beta}   - \\boldsymbol{\\beta}^T X^T y ) $$ \n",
    "\n",
    "We can cut down the first term, since it does not depend on $\\boldsymbol{\\beta}$, and take the following rules into consideration:\n",
    "\n",
    "* $ \\Delta_{\\boldsymbol{\\theta}} (a^T \\theta) = a$\n",
    "* $ \\Delta_{\\boldsymbol{\\theta}} (\\theta^T a) = a$\n",
    "* $ \\Delta_{\\boldsymbol{\\theta}} (\\theta^T A \\theta) = 2A\\theta$\n",
    "\n",
    "And so we get:\n",
    "\n",
    "$$ \\Delta_{\\boldsymbol{\\beta}} J  = (-(y^T X)^T  + 2  X^T X \\boldsymbol{\\beta} - X^T y ) $$ \n",
    "\n",
    "$$ \\Delta_{\\boldsymbol{\\beta}} J  = (-2 X^T y  + 2  X^T X \\boldsymbol{\\beta} ) $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, equaling the derivative to zero, we get a very clean solution:\n",
    "\n",
    "$$ 0 = (-2 X^T y  + 2  X^T X \\boldsymbol{\\beta} ) $$ \n",
    "\n",
    "$$ X^T X \\boldsymbol{\\beta} = X^T y $$ \n",
    "\n",
    "$$ \\boldsymbol{\\beta} = (X^T X)^{-1} X^T y $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
