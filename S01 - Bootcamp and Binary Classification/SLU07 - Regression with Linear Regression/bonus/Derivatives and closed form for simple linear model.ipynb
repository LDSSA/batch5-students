{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus notebook - derivatives and closed form for simple linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1) Error function:\n",
    "\n",
    "Remember our error function definition, from the learning notebook: \n",
    "\n",
    "$$J(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $$ \n",
    "\n",
    "Expanding with our linear model, we get:\n",
    "\n",
    "$$J(y, \\hat{y}) = \\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Derivatives of error function for simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 ) Intercept derivative:\n",
    "\n",
    "We'll now derivate with respect to $\\beta_0$. We can write the initial formulation:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{d}{d \\beta_0} (\\frac{1}{N}\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2) $$\n",
    "\n",
    "Then, expanding the square, we get:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{d}{d \\beta_0} (\\frac{1}{N}\\sum_{i=1}^N (y_i^2 - 2 y_i \\beta_0 - 2 y_i \\beta_1 x_i + 2 \\beta_0\\beta_1 x_i + \\beta_1^2 x_i^2 + \\beta_0^2 )) $$\n",
    "\n",
    "The derivative of the sum is just the sum of the derivative:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N (\\frac{d}{d \\beta_0} y_i^2 - \\frac{d}{d \\beta_0} 2 y_i \\beta_0 - \\frac{d}{d \\beta_0} 2 y_i \\beta_1 x_i + \\frac{d}{d \\beta_0} 2 \\beta_0\\beta_1 x_i + \\frac{d}{d \\beta_0} \\beta_1^2 x_i^2 + \\frac{d}{d \\beta_0} \\beta_0^2 ) $$\n",
    "\n",
    "So now we can cut all the terms that do not depend on $\\beta_0$, leading to:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N (0 - 2 y_i - 0 + 2 \\beta_1 x_i + 0 + 2\\beta_0)  \\\\\n",
    "\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N (-2 y_i + 2 \\beta_1 x_i + 2\\beta_0) $$\n",
    "\n",
    "Finally, we'll rearange the interior of the sum and get to:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = -\\frac{1}{N} \\sum_{i=1}^N [2 (y_i - \\beta_1 x_i - \\beta_0)] $$\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = -\\frac{1}{N} \\sum_{i=1}^N [2 (y_i - \\hat{y_i})] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 ) Coefficient derivative:\n",
    "\n",
    "We'll now derivate with respect to $\\beta_1$. Once again, let's write the initial formulation and the expansiong of the internal square:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = \\frac{d}{d \\beta_1} (\\frac{1}{N}\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2) \\\\\n",
    "\\frac{d J}{d \\beta_1} = \\frac{1}{N}\\sum_{i=1}^N (\\frac{d}{d \\beta_1} y_i^2 - \\frac{d}{d \\beta_1} 2 y_i \\beta_0 - \\frac{d}{d \\beta_1} 2 y_i \\beta_1 x_i + \\frac{d}{d \\beta_1} 2 \\beta_0\\beta_1 x_i + \\frac{d}{d \\beta_1} \\beta_1^2 x_i^2 + \\frac{d}{d \\beta_1} \\beta_0^2 ) $$\n",
    "\n",
    "This time we'll cut the terms that do not depend on the coefficient $\\beta_1$, leading to:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = \\frac{1}{N}\\sum_{i=1}^N (0 - 0 - 2 y_ix_i + 2 \\beta_0 x_i + 2 \\beta_1 x_i^2 + 0)  \\\\\n",
    "\\frac{d J}{d \\beta_1} = \\frac{1}{N}\\sum_{i=1}^N (- 2 y_ix_i + 2 \\beta_0 x_i + 2 \\beta_1 x_i^2) $$\n",
    "\n",
    "And finally, we simplify the expression:\n",
    "\n",
    "$$ \\frac{d J}{d \\beta_1} = -\\frac{1}{N}\\sum_{i=1}^N [2( y_i - \\beta_0 - \\beta_1 x_i)x_i] \\\\\n",
    " \\frac{d J}{d \\beta_1} = -\\frac{1}{N}\\sum_{i=1}^N [2( y_i - \\hat{y_i})x_i] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Closed form solution for simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to the closed form solution, we will basically equal our derivatives, computed above, to zero:\n",
    "\n",
    "### 3.1 ) Finding the intercept:\n",
    "\n",
    "To find the intercept, we'll equal its derivative to zero:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = 0$$\n",
    "$$-\\frac{1}{N} \\sum_{i=1}^N [2( y_i - \\beta_0 - \\beta_1 x_i)] = 0$$\n",
    "\n",
    "We can start by cutting any terms multiplying or dividing the overall equation, since we are equaling to zero.\n",
    "\n",
    "$$\\sum_{i=1}^N [( y_i - \\beta_0 - \\beta_1 x_i)] = 0$$\n",
    "\n",
    "We will then split the sums so we can isolate the weight term:\n",
    "\n",
    "$$\\sum_{i=1}^N y_i - \\sum_{i=1}^N \\beta_0  -  \\sum_{i=1}^N \\beta_1 x_i = 0$$\n",
    "\n",
    "Rearranging the terms, we get:\n",
    "\n",
    "$$\\sum_{i=1}^N y_i - \\sum_{i=1}^N \\beta_1 x_i = \\sum_{i=1}^N \\beta_0 $$\n",
    "\n",
    "We can move all factors outside of the sums:\n",
    "\n",
    "$$\\sum_{i=1}^N y_i - \\beta_1 \\sum_{i=1}^N x_i = \\beta_0 \\sum_{i=1}^N 1 $$\n",
    "$$\\sum_{i=1}^N y_i - \\beta_1 \\sum_{i=1}^N x_i = \\beta_0 N $$\n",
    "\n",
    "\n",
    "Finally, we isolate the weight completely:\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N y_i - \\beta_1 \\frac{1}{N}\\sum_{i=1}^N x_i = \\beta_0 $$\n",
    "\n",
    "Notice that $\\frac{1}{N}\\sum_{i=1}^N y_i$ and $\\frac{1}{N}\\sum_{i=1}^N x_i$ are just averages over the samples' labels and inputs, respectively, so we get:\n",
    "\n",
    "$$\\bar{y} - \\beta_1 \\bar{x} = \\beta_0 $$\n",
    "\n",
    "Notice that the equation we got to depends on the value of the coefficient, so let's proceed to compute the solution for that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ) Finding the coefficient:\n",
    "\n",
    "To find the coefficient, we'll follow the first steps as above, equaling the derivative to zero and cutting out factors that affect the global equation:\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = 0$$\n",
    "$$-\\frac{1}{N} \\sum_{i=1}^N [2( y_i - \\beta_0 - \\beta_1 x_i)x_i] = 0$$\n",
    "$$\\sum_{i=1}^N [( y_i - \\beta_0 - \\beta_1 x_i)x_i] = 0$$\n",
    "\n",
    "We can then replace $\\beta_0$ from the result we got before:\n",
    "\n",
    "$$\\sum_{i=1}^N [( y_i - \\bar{y} + \\beta_1 \\bar{x} - \\beta_1 x_i)x_i] = 0$$\n",
    "$$\\sum_{i=1}^N [( y_i - \\bar{y})x_i - \\beta_1 ( x_i - \\bar{x})x_i] = 0$$\n",
    "\n",
    "\n",
    "We can then split the sums and move the coefficient to the other side of the equation:\n",
    "\n",
    "$$\\sum_{i=1}^N [( y_i - \\bar{y})x_i] - \\beta_1 \\sum_{i=1}^N[( x_i - \\bar{x})x_i] = 0$$\n",
    "$$\\sum_{i=1}^N [( y_i - \\bar{y})x_i] = \\beta_1 \\sum_{i=1}^N[( x_i - \\bar{x})x_i]$$\n",
    "$$\\frac{\\sum_{i=1}^N [( y_i - \\bar{y})x_i]}{\\sum_{i=1}^N[( x_i - \\bar{x})x_i]} = \\beta_1 $$\n",
    "$$\\frac{\\sum_{i=1}^N(y_ix_i - \\bar{y}x_i)}{\\sum_{i=1}^N(x_i^2 - \\bar{x}x_i)} = \\beta_1 $$\n",
    "\n",
    "Notice that $\\sum_{i=1}^N y_i = N\\bar{y}$ and $\\sum_{i=1}^N x_i= N\\bar{x}$:\n",
    "\n",
    "$$\\frac{\\sum_{i=1}^N y_ix_i - \\bar{y}\\bar{x}}{\\sum_{i=1}^Nx_i^2 - \\bar{x}^2} = \\beta_1 $$\n",
    "\n",
    "You can either take this as the final form or do one more step. This manipulation is not as straightforward, but it eventually leads to a simplification that can be usefull in certain situations. Note that:\n",
    "\n",
    "$\\sum_{i=1}^N (\\bar{x}^2 - x_i\\bar{x}) = 0 $\n",
    "\n",
    "$\\sum_{i=1}^N (\\bar{x}\\bar{y} - y_i\\bar{x}) = 0 $\n",
    "\n",
    "So we can sum these on our fraction without altering the quantities:\n",
    "\n",
    "$$\\frac{\\sum_{i=1}^N (y_ix_i - \\bar{y}x_i) + \\sum_{i=1}^N (\\bar{x}\\bar{y} - y_i\\bar{x})}{\\sum_{i=1}^N(x_i^2 - \\bar{x}x_i) + \\sum_{i=1}^N (\\bar{x}^2 - x_i\\bar{x})} = \\beta_1 $$\n",
    "\n",
    "And then the equation for the coefficient can be rewritten as a ratio between the covariance and variance of the sample:\n",
    "\n",
    "$$\\beta_1 = \\frac{\\frac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\frac{1}{N}\\sum_{i=1}^N(x_i - \\bar{x})^2} = \\frac{cov(x,y)}{var(x)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
