{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66f2958",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bf24e7ba2c45b1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU02 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd525da6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bebba7f87f4f151b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2b87c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a23783765364606a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1 Read the Programs data (graded)\n",
    "\n",
    "In this first exercise, we aim to create a single dataframe, combining all programs from all seasons.\n",
    "\n",
    "With a caveat though: **we want to include seasons after 1950**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539f29e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e16abeb47fc4ea37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_season(folder_path, file_name):\n",
    "    path = os.path.join(folder_path, file_name)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def read_programs(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    # Create a list with the name of all files containing programs from\n",
    "    # 1950 inclusive and onwards (just the filename, no complete path.)\n",
    "    # files_after_1950: List[str] = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Create a list with the dataframes\n",
    "    # seasons: List[pd.DataFrame] = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Use pd.concat to create a single dataframe.\n",
    "    # programs: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Drop the column GUID.\n",
    "    # programs = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## Remove Duplicated lines.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Set the index to be the column ProgramID\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return programs\n",
    "\n",
    "programs = read_programs(os.path.join('data', 'programs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b3ffb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2656708135a5a5e4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert programs['Season'].min() == '1950-51'\n",
    "assert programs['Season'].max() == '2016-17'\n",
    "assert programs.index.name == 'ProgramID'\n",
    "assert programs.shape == (7341, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31920670",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3383047fa18f453e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Read the Concerts data (graded)\n",
    "\n",
    "Although we list all transformations step-by-step for the sake of clarity, we expect you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f86c9d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e228c2d82463c6b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_concerts(file_path): \n",
    "    # Read concerts data and drop the GUID column.\n",
    "    # concerts: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Remember to_datetime? We need here. We need to parse the columns Date and \n",
    "    # Time. Use pd.to_datetime(...).dt.date for the Date and pd_to_datetime(..., \n",
    "    # format=%I:%M%p).dt.time for the Time.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## Remove Duplicated lines.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return concerts\n",
    "\n",
    "concerts = make_concerts(os.path.join('data','concerts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a356677",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eecdd1635541055d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "concerts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0b5f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3d21007e725ab889",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert concerts.shape == (21582, 7)\n",
    "assert concerts.Date.min() == datetime.date(1842, 12, 7)\n",
    "assert concerts.Date.max() == datetime.date(2017, 7, 7)\n",
    "assert set(concerts.columns) == set([\n",
    "    'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue', 'Date', 'Time'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6c904",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ae195e6dd100fc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 Combine Programs and Concerts data (graded)\n",
    "\n",
    "Let's combine both dataframes into a single dataset, using an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a56101",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a65f1464b4525a8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember that you want to join on the index of one of the dataframes.\n",
    "# Join only the concerts with valid ProgramIDs\n",
    "# nyp = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481b7af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a02c31772c0f71d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nyp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26483891",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ac9aef3d5251e36c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert nyp.shape == (13254, 9)\n",
    "assert set(nyp.columns) == set([\n",
    "    'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue',\n",
    "    'Date', 'Time', 'Orchestra', 'Season'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71271ecf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fd4cd11d5139889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4 Read Works and Soloists data (graded)\n",
    "\n",
    "We will read the two remaining pieces of data. \n",
    "\n",
    "Again, albeit the step-by-step description, we encourage you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb533fb6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-503e208490ff38e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_works(file_path):\n",
    "    # Read the works data.\n",
    "    # works: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Remove the Intervals (attention to the values in the isInterval column).\n",
    "    # works: pd.DataFrame = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # Select the columns ProgramID, WorkID, ComposerName, WorkTitle, Movement and ConductorName.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## Remove Duplicated lines.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return works\n",
    "\n",
    "\n",
    "def read_soloists(file_path):\n",
    "    # Read the soloists data and drop GUID and MovementID Columns\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ## Remove Duplicated lines.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return soloists\n",
    "\n",
    "\n",
    "works = read_works('data/works.csv')\n",
    "soloists = read_soloists('data/soloists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f60f07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8389314995f18ea",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert works.shape == (71065, 6)\n",
    "assert set(works.columns) == set([\n",
    "    'ProgramID', 'WorkID', 'ComposerName', 'WorkTitle', 'Movement', 'ConductorName'\n",
    "])\n",
    "\n",
    "assert soloists.shape == (50292, 5)\n",
    "assert set(soloists.columns) == set([\n",
    "   'ProgramID', 'WorkID', 'SoloistName', 'SoloistInstrument', 'SoloistRole'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839081fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c16e4e26e68cd019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 Combine Works and Soloists (graded)\n",
    "\n",
    "Like we did for Programs and Concerts, now we combine Works and Soloists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae5297",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-08d9a086cc5646cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Combine both dataframes, again using an inner type of join. An work is identified by the pair\n",
    "# ProgramId, WorkID\n",
    "# works_and_soloists : pd.DataFrame = ....\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184d4cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4d9f103dfffd311b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert works_and_soloists.shape == (64698, 9)\n",
    "assert set(works_and_soloists.columns) == set(\n",
    "    [\n",
    "        'ProgramID', 'WorkID', 'ComposerName', 'WorkTitle', 'Movement',\n",
    "        'ConductorName', 'SoloistName', 'SoloistInstrument', 'SoloistRole'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddd282",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab79800d6e447f1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6 Combine everything (graded)\n",
    "\n",
    "The final goal here is to create a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4eee72",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce1f05022e8cd63a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Combine works_and_soloists and nyp into a single dataframe.\n",
    "# nyp_merged = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c7dc60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ce29fa3aec1c244e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert nyp_merged.shape == (72185, 17)\n",
    "assert set(nyp_merged.columns) == set(\n",
    "    [\n",
    "       'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue', 'Date',\n",
    "       'Time', 'Orchestra', 'Season', 'WorkID', 'ComposerName', 'WorkTitle',\n",
    "       'Movement', 'ConductorName', 'SoloistName', 'SoloistInstrument',\n",
    "       'SoloistRole'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c7125",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4407d9518b0d6e2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 7 Final transformations (graded)\n",
    "\n",
    "Now, we perform the train-test split.\n",
    "\n",
    "We also perform some final transformations on both datasets:\n",
    "\n",
    "* Include some date features: Year, Month, Hour, Minute, Day and Weekday\n",
    "* Create a new feature, ComposerLastName from ComposerName column. \n",
    "* Filter out rows that have a location that appear is less than 10 times in the DataFrame.\n",
    "* Drop ProgramID, ConcertID, WorkID, Date and Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5094ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1c1ab0d912e615ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def append_date_features(df):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df\n",
    "\n",
    "def append_composer_last_name(df):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # You should follow these exact steps:\n",
    "    #   1 - Include some date features: Year, Month, Hour, Minute, Day and Weekday\n",
    "    #   2 - Create a new feature, ComposerLastName from ComposerName column. \n",
    "    #   3 - Filter out rows that have a location that appear is less than 10 times in the DataFrame.\n",
    "    #   4 - Drop ProgramID, ConcertID, WorkID, Season, Date, Time\n",
    "    #   \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df\n",
    "\n",
    "\n",
    "nyp_preprossed = preprocess_data(nyp_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21e1d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9d9c75c48e4eaf63",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert nyp_preprossed.shape == (71659, 18)\n",
    "assert set(nyp_preprossed.columns) == {\n",
    "       'EventType', 'Location', 'Venue', 'Orchestra',\n",
    "       'ComposerName', 'WorkTitle', 'Movement', 'ConductorName', 'SoloistName',\n",
    "       'SoloistInstrument', 'SoloistRole', 'Year', 'Month', 'Day', 'Hour',\n",
    "       'Minute', 'Weekday', 'ComposerLastName'\n",
    "}\n",
    "assert nyp_preprossed.groupby('Location').size().min() == 10\n",
    "assert nyp_preprossed.ComposerLastName.value_counts().loc['Mozart'] == 3333\n",
    "assert nyp_preprossed.ComposerLastName.value_counts().loc['Gershwin'] == 2127\n",
    "assert nyp_preprossed.ComposerLastName.nunique() == 1241"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4918d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4557c57da6142038",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The house prices dataset\n",
    "\n",
    "A dataset containing several characteristics of several houses and their sell price \n",
    "\n",
    "* LotFrontage: Linear feet of street connected to property\n",
    "* LotArea: Lot size in square feet\n",
    "* OverallQual: Rates the overall material and finish of the house\n",
    "       10  Very Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\n",
    "       5\tAverage\n",
    "       4\tBelow Average\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "* OverallCond: Rates the overall condition of the house\n",
    "\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\t\n",
    "       5\tAverage\n",
    "       4\tBelow Average\t\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "* MasVnrArea: Masonry veneer area in square feet\n",
    "* BsmtFinSF1: Type 1 finished square feet\n",
    "* BsmtUnfSF: Unfinished square feet of basement area\n",
    "* TotalBsmtSF: Total square feet of basement area\n",
    "* 1stFlrSF: First Floor square feet\n",
    "* 2ndFlrSF: Second floor square feet\n",
    "* LowQualFinSF: Low quality finished square feet (all floors)\n",
    "* GrLivArea: Above grade (ground) living area square feet\n",
    "* BsmtFullBath: Basement full bathrooms\n",
    "* BsmtHalfBath: Basement half bathrooms\n",
    "* FullBath: Full bathrooms above grade\n",
    "* HalfBath: Half baths above grade\n",
    "* BedroomAbvGr: Bedrooms above grade (does NOT include basement bedrooms)\n",
    "* KitchenAbvGr: Kitchens above grade\n",
    "* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "* Fireplaces: Number of fireplaces\n",
    "* GarageCars: Size of garage in car capacity\n",
    "* GarageArea: Size of garage in square feet\n",
    "* WoodDeckSF: Wood deck area in square feet\n",
    "* OpenPorchSF: Open porch area in square feet\n",
    "* EnclosedPorch: Enclosed porch area in square feet\n",
    "* 3SsnPorch: Three season porch area in square feet\n",
    "* ScreenPorch: Screen porch area in square feet\n",
    "* PoolArea: Pool area in square feet\n",
    "* MiscVal: $Value of miscellaneous feature \n",
    "* SellingDate: Date when the house was sold\n",
    "* BuildingDate: Date when the house was built\n",
    "* RemodAddDate: Remodel date (same as construction date if no remodeling or additions)\n",
    "* SalePrice: The house price at the selling date (our target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fd662",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7582e93ac7adb85f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def house_price_dataset():\n",
    "    return pd.read_csv(\n",
    "    'data/housePrices.csv', \n",
    "        parse_dates=[\n",
    "            'SellingDate',\n",
    "            'BuildingDate',\n",
    "            'RemodAddDate'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = house_price_dataset()\n",
    "dataset_train, dataset_test = train_test_split(dataset, random_state=0)\n",
    "X_train = dataset_train.drop(columns='SalePrice')\n",
    "y_train = dataset_train.SalePrice\n",
    "X_test = dataset_test.drop(columns='SalePrice')\n",
    "y_test = dataset_test.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa1fe4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-425695b0c9d45ae9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 8 Build a DateTransformer transformer (graded)\n",
    "\n",
    "There's a simple transformer that can be useful, from times to times, when modeling.\n",
    "\n",
    "What we want is to build a transformer that transform dates into timedeltas.\n",
    "\n",
    "Usually when you have features that are Dates you compute a time delta between the feature and a given refence date.\n",
    "\n",
    "e.g Imagine that your clients have a loyalty period that ends at a given date. When your model is doing some predictions, one of the features that you can use is the number of days until the end of the loyalty period. i.e the date when the loyalty ends minus the date when your model is running. \n",
    "\n",
    "In the house prices dataset, the selling date will be the reference data, since we want to predict the house price at the selling date. Then we need to convert the other date using our transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b78c45",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9538038181ba578",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Implement the __init__ method.\n",
    "    # Our DateTransformer must be able to receive two parameters: \n",
    "    # datetime_cols: a list, that contains the datetime cols that should be converted\n",
    "    # ref_date_col - indicates the name of the column that should be used as reference date,\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "    # There's no need for a fit method in this case, it does nothing.\n",
    "    # We should be able to call fit without any explicit parameters.\n",
    "    # Meaning: we should be able to call transformer.fit().\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Transform should transform all datetime columns into the difference in days to the reference date.\n",
    "    # The reference date column should be dropped. \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1d26c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b64b26753ecd8561",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_trasformed = DateTransformer(\n",
    "    datetime_cols=['BuildingDate', 'RemodAddDate'], \n",
    "    ref_date_col='SellingDate'\n",
    ").fit_transform(X_train)\n",
    "assert X_train_trasformed.BuildingDate.min() == -49008\n",
    "assert X_train_trasformed.BuildingDate.max() == -1\n",
    "assert 'SellingDate' not in X_train_trasformed.columns\n",
    "assert X_train_trasformed.dtypes.BuildingDate == np.dtype('int64')\n",
    "assert X_train_trasformed.dtypes.RemodAddDate == np.dtype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714780de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46fe8c71f80d2717",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 9 Building the pipeline (graded)\n",
    "\n",
    "Finally, we want to use the two transformers together and run a linear regression on top.\n",
    "\n",
    "We want to Covert hour dates to time deltas relative to the Selling Date.\n",
    "\n",
    "We want to scale all features to the same range, using `sklearn.preprocessing.StandardScaler()`.\n",
    "\n",
    "We want to estimate the SellingPrice using a Liner Regression.\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0afe11",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bc09de5e71383d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline including:\n",
    "#   1 - 'date_converter', DateTransformer(['BuildingDate', 'RemodAddDate'], ref_date_col='SellingDate')\n",
    "#   2 - 'standard_scaller', StandardScaler() with the default parameters\n",
    "#   3 - 'model', LinearRegression\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('MAE: {}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01715551",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5d21dcdd34a13d24",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(pipeline) == Pipeline\n",
    "assert type(pipeline.named_steps['date_converter']) == DateTransformer\n",
    "assert type(pipeline.named_steps['standard_scaller']) == StandardScaler\n",
    "assert pipeline.named_steps['date_converter'].get_params()['ref_date_col'] == 'SellingDate'\n",
    "assert set(\n",
    "    pipeline.named_steps['date_converter'].get_params()['datetime_cols']\n",
    ") == {'BuildingDate', 'RemodAddDate'}\n",
    "assert type(pipeline.named_steps['model']) == LinearRegression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3facfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57f2ca220627e218",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Exercises complete, congratulations! You are about to become a certified data wrangler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
