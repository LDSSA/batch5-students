{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Recommender Systems\n",
    "\n",
    "Over *SLU10 - Metrics for Regression* and *SLU11 - Metrics for Classification* the metrics for regression and classification were explored, respectively. What kind of metrics should we use to evaluate the performance of our recommendations? Regression metrics? Classification metrics? It depends on what is the goal of the analysis and how the data is processed.\n",
    "\n",
    "The three types of metrics available for recommender evaluations are:\n",
    "\n",
    "1. Regression metrics\n",
    "\n",
    "2. Classification metrics\n",
    "\n",
    "3. Information Retrieval metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Regression Metrics\n",
    "\n",
    "It is possible to conceptualize a recommendation problem as a regression problem if the objective of the analysis is to predict the value of an numerical indicator, such as ratings. If you want to predict the value of a given indicator for a specific item and user, you can - theoretically - solve it as a regression problem. By defining users' and items variables as independent variables and the respective rating as the dependent variable (like the example below), it is possible to solve it with regression models. \n",
    "\n",
    "Consider the case where movie rating data is processed to obtain the following table:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"./media/regression.png\" width=\"650\" /></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The users' and items' variables are the independent variables and the rating is the dependent variable and thus it is possible to use regression models to predict the movie rating for a specific user. The results can be evaluated using regression metrics as discussed in *SLU10 - Metrics for Regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Classification Metrics\n",
    "\n",
    "Identically, a recommendation problem can be reframed as a classification problem if the target variable is categorical.\n",
    "\n",
    "If the goal is to predict if an user would recommend a given item, the data can be processed as before and a classification model could be train to make predicitons. The quality of these predictions can be evaluated using classification metrics as described in *SLU11 - Metrics for Classification*.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"./media/classification.png\" width=\"650\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Information Retrieval\n",
    "\n",
    "From [*Introduction to Information Retrieval*](https://www-nlp.stanford.edu/IR-book/):\n",
    "\n",
    "\"Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).\" \n",
    "\n",
    "Basically when using search engines or asking for advice from a librarian, we are engaging in IR. We are using search terms to perform a query over a large collection of resources. We aren't going into much detail on IR. What is important to us is how a recommender system compares to an Information Retrieval System (IRS) and how IR evaluates the accuracy of the results.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"./media/IRS_RS.png\" width=\"700\" /></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "In simple terms, an IRS takes an user query and tries to return documents that contain relevant information. Parallelly, a recommender system tries to find items that satisfy the user. IR usually deals with queries created by the user while with RS there isn't a direct query from the user. \n",
    "\n",
    "\n",
    "We can think of it in the following matter: a Recommender System is an Information Retrieval system that answers the query \"What items best satisfy this user?\".\n",
    "The RS will provide the results of this query (the recommendations) and the user will provide feedback by accessing (or not) the items or by providing a rating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IR metrics are used to compare the results provided by the IRS and the expected results for the given query.\n",
    "Two factors indicate how good the IR results are: rank and relevance.\n",
    "\n",
    "The IR results are provided as a ranked list of documents starting with the documents that best fit the query criteria to the documents that least fit the criteria. \n",
    "Consider the example where we use a search engine to find web pages regarding regression - as in regression analysis. For us, a web page with detailed information on regression analysis is more useful than a page that simple states the definition of regression or a page about age regression or the movie *Regression (2015)*. This degree of \"usefulness\" can be used to rank the results - provided there is feedback. \n",
    "\n",
    "A relevant document (or result) is a document that is expected by the user. Using the previous example, the pages about age regression and the movie are not relevant to the user. Having increased number of non-relevant results is an indicator of poor IR performance.\n",
    "\n",
    "When RS returns a ranked list of results just like IR, it is possible to use IR metrics to evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Information Retrieval Evaluation Metrics for top-*N* Lists\n",
    "\n",
    "\n",
    "Let's start by introducing some nomenclature with a dummy example. Imagine an user that has preference for fruits. We develop a RS model that creates a list of recommendations ranked from most recommended to least recommended, resulting in:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"./media/IR_sets.png\" width=\"800\" /></center>\n",
    "<center> The sets' elements are ordered by order of preference.</center>\n",
    "<br>\n",
    "\n",
    "The first row represents the actual user preferences (as is avaliable on test data) ranked by order of preference and the second row represents the recommendations also sorted by predicted preference. The set of all user preferences is represented as $\\{Relevant\\ Items\\}_u$ and the set of all recommendations is represented as $\\{Retrieved\\ Items\\}_u$. The relevant items are highlighted in green and the non-relevant items (and therefore not present in $\\{Relevant\\ Items\\}_u$) are highlighted in red.\n",
    "\n",
    "\n",
    " Additionally $|{set}|$ is the __cardinality__ of the set, which in this case is the number of items on the set. The operator $\\cap$ represents the __intersection__ between two sets. The result of the intersection between two sets is another set whose elements are present in both original sets.\n",
    "\n",
    "With this out of the way, let's define some evaluation metrics.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Precision\n",
    "\n",
    "Precision measures how many recommended items to the user $u$ are relevant.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Precision(\\{Retrieved\\ Items\\}_u) = \\frac{|\\{Retrieved\\ Items\\}_u \\cap \\{Relevant\\ Items\\}_u |}{|\\{Retrieved\\ Items\\}_u|}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "To evaluate the RS as a whole, we average the precision for all active users $u \\in U$, where $U$ is the set containing all users.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Precision(\\{Retrieved\\ Items\\}) = \\frac{\\sum\\limits_{u \\in U} Precision(\\{Retrieved\\ Items\\}_u)}{|U|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Recall\n",
    "\n",
    "Recall, on the other side, relates to how many relevant items were recommended, out of all relevant items for the user $u$.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Recall(\\{Retrieved\\ Items\\}_u) = \\frac{|\\{Retrieved\\ Items\\}_u \\cap \\{Relevant\\ Items\\}_u |}{|\\{Relevant\\ Items\\}_u|}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Again, to evaluate the TS we average the results of all active users $u \\in U$.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Recall(\\{Retrieved\\ Items\\}) = \\frac{\\sum\\limits_{u \\in U} Recall(\\{Retrieved\\ Items\\}_u)}{|U|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Average Precision (AP)\n",
    "\n",
    "Precision and recall ignore the rank of both relevant and recommended items. A ranking metric may be more appropriated.\n",
    "\n",
    "To understand average precision, we must start with Precision@k and Recall@k, i.e., precision and recall up to cut-off $k$.\n",
    "\n",
    "In other words, we consider only the subset of recommendations $\\{Retrieved\\ Items\\}_u^k \\subset \\{Retrieved\\ Items\\}_u$ from rank 1 through rank $k \\leqslant N$, with $N$ being the total number of retrieved items $|\\{Retrieved\\ Items\\}_u|$.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Precision@k(\\{Retrieved\\ Items\\}_u) = \\frac{|\\{Retrieved\\ Items\\}_u^k \\cap \\{Relevant\\ Items\\}_u |}{|\\{Retrieved\\ Items\\}_u^k|}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Recall@k(\\{Retrieved\\ Items\\}_u) = \\frac{|\\{Retrieved\\ Items\\}_u^k \\cap \\{Relevant\\ Items\\}_u |}{|\\{Relevant\\ Items\\}_u|}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The AP is a ranking metric, measuring the frequency of relevant recommendations.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$AP@N(\\{Retrieved\\ Items\\}_u) = \\frac{\\sum\\limits_{k = 1}^N (Precision@k(\\{Retrieved\\ Items\\}_u) \\cdot relevant(k^{th})}{|\\{Relevant\\ Items\\}_u|}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The $relevant(k^{th})$ bit is a boolean value, indicating whether the $k$-th element is relevant, or not.\n",
    "\n",
    "AP values how many correct recommendations $|\\{Retrieved\\ Items\\}_u^k \\cap \\{Relevant\\ Items\\}_u|$ we have up to the rank $k$, out of all recommendations $|\\{Retrieved\\ Items\\}_u^k|$. It increases only with correct recommendations and ignores non-relevant items. Early hits, i.e., front-loading correct recommendations, carry over and are continuously rewarded. Finally, the AP can never decrease as you increase $N$.\n",
    "\n",
    "There is an alternative formula for AP, in terms of both precision and the change in recall from the subset $k$ − 1 to the $k$-th.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$AP@N(\\{Retrieved\\ Items\\}_u) = \\sum\\limits_{k=1}^NPrecision@k(\\{Retrieved\\ Items\\}_u) * \\Delta Recall@k(\\{Retrieved\\ Items\\}_u)$$ \n",
    "\n",
    "<br>\n",
    "\n",
    "From this formula one can interpret AP as the area under the Precision vs Recall Curve (AUPRC), alike AUROC being the area under the true positive rate (TPR, Recall) vs the false positive rate (FPR) curve. It measures the capacity of the model to identify many relevant items without select many non-relevant items. Note that the value of the AUPRC for a no skill model <b>is equal the proportion of positive cases</b> when $N$ is equal to the number of all available items([Source](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)). This contrasts with the fact that AUROC of a no skill model is always 0.5 regardless of the proportion of positive cases. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<center><img src=\"./media/AUPRC_balanced.png\" width=\"500\" /></center>\n",
    "<center> A Precision-Recall curve for a Logistic Regression on a <b>balanced</b> dataset <a href=\"https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\">(Source)</a>. </center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The AUPRC can be particularly useful when the fraction of positive cases is small and when identifying a positive case is more important than identifying negative cases. These conditions typically happen in medical applications where the incidence of a condition is low - low number positive cases - and is more important to identify the condition than to verify that the pacient does not have the condition. In RS these conditions also apply. The users can only access a tiny amount of all available items; just consider the amount of books, movies, series, products, etc. The amount of relevant items is, therefore, significantly smaller than non-relevant items. Furthermore, it is more important to recommend relevant items than to make sure that non-relevant items are not recommended. The user might even like the - supposedly - non-relevant item afterall. Take the case of [Plastic Love - Mariya Takeuchi](https://www.youtube.com/watch?v=3bNITQR4Uso). In 2018, the youtube recommendation algorithm \"spontaneously\" recommended a 80's pop song to a massive number of users that were completely unaware of the City Pop genre. This sparked the resurgence of an entire music genre ([Source](https://www.youtube.com/watch?v=PlPTXR7e6As)).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"./media/failed_success.jpeg\" width=\"400\" /></center>\n",
    "<center> When recommending something weird and the user likes it.</center>\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Mean Average Precision (mAP)\n",
    "\n",
    "The Average Precision (AP) is further averaged over all users and reported as a single score.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$mAP@N(\\{Retrieved\\ Items\\}) = \\frac{\\sum\\limits_{u \\in U} AP@N(\\{Retrieved\\ Items\\}_u)}{|U|}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This way, we use a metric that considers both the number and the ranking of hits, i.e., useful recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Spearman's rank correlation coefficient\n",
    "\n",
    "Another approach is to consider the rank of the relevant items as one variable and the rank of the recommendations as another variable. In this case it is appropriated to use rank correlation coefficients to evaluate the **ordinal** relationship between them. One of these rank correlation coefficients was already discussed on SLU05 - Covariance and Correlation: the **Spearman's rank correlation coefficient, $r_s$**. \n",
    "\n",
    "To calculate the rank correlation, the **rank** of each item on the $\\{Relevant\\ Items\\}_u$ is compared with the **rank of the same item** on $\\{Retrieved\\ Items\\}_u$. We can define two rank variables: User's Preferences Ranks (*PR*) and Recommendations Ranks (*RR*), where the user's and recommendations' ranks for each item are stored, respectively. The overall order of the ranks in these variables is not important. What matters is that the **position of an item has to be the same on both variables. If the first element of PR is the user's preferences rank of \"Item X\" then the first element of RR has be the recommendations rank of \"Item X\" and so forth.**\n",
    "\n",
    "Considering the case above, we can identify the rank of each item of $\\{Relevant\\ Items\\}_u$ and $\\{Retrieved\\ Items\\}_u$. By reording the RR\n",
    "to have the items on the same position as PR it is easier to see matching items can have different rank value between PR and RR. For instance, the item \"apple\" is the user's second (2) preferred item while it is the fifth (5) recommended item.\n",
    "<br>\n",
    "\n",
    "<center><img src=\"./media/spearman_table.png\" width=\"800\" /></center>\n",
    "<center>The items' rank is written within paranthesis ().</center>\n",
    "<br>\n",
    "\n",
    "The Pearson correlation between these two variables returns the Spearman's rank correlation coefficient between recommendations and the user's preference.\n",
    "When the all ranks are distinct integers - such as with our case - the Spearman's rank correlation coefficient can calculated with the formula:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ r_s = 1- \\frac{6 \\sum\\limits_{i=1}^{n} d_i^2}{n(n-1)}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $i$ iterates over the items, $n$ is the number of ranks (items) to be compared and $d_i = PR_i - RR_i$ is the difference of ranks for item $i$. In our case $n$ can be the cut off $k$ on the number of relevant items. \n",
    "\n",
    "The Spearman's rank correlation coefficient can have values ranging between -1 and 1. Correlation of 1 indicates that the order of the ranks in both variables are exactly the same, while correlation of -1 indicates that the variables have fully opposed order. Correlation of 0 indicates that the variables are completely independent. One should note that the computation of the correlation is vulnerable to the presence of missing items, mainly in $\\{Retrieved\\ Items\\}_u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "There are a lot more metrics that can be used to evaluate the outcomes of recommender systems. The preference from one metric to another depends on multiple factors such as output type (binary, ordinal, recommendations), the type of users' feddback, business requirements and ease of interpretation. We have explored some of the common metrics for recommender systems. It is encouraged to explore other metrics. The best metric for a specific context might not be helpful for a dissimilar context.\n",
    "\n",
    "## Time to practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [*Short introduction to Information retrieval*](https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_information_retrieval.htm)\n",
    "\n",
    "- [*Introduction to Information Retrieval*](https://nlp.stanford.edu/IR-book/), more specifically the [*chapter 8*](https://nlp.stanford.edu/IR-book/pdf/08eval.pdf) on evaluation.\n",
    "\n",
    "- [*Information Retrieval Wiki*](https://en.wikipedia.org/wiki/Information_retrieval)\n",
    "\n",
    "- [*Information Retrieval Metrics Wiki*](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))\n",
    "\n",
    "- [*Precision-Recall Curves in Python*](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)\n",
    "\n",
    "- [*More info on AUPRC*](https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/)\n",
    "\n",
    "- [*Rank Correlation Wiki*](https://en.wikipedia.org/wiki/Rank_correlation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BLU12_B5_3.7_jupyter_venv",
   "language": "python",
   "name": "blu12_b5_3.7_jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
