{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Information Extraction\n",
    "\n",
    "Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. Information Extraction may be presented in three subtasks:\n",
    "\n",
    "* **Named Entity Recognition**, retrieve entities (like Person, Location, etc.) in the text. \n",
    "* **Relation Extraction**, find the relation between two entities in the text.\n",
    "* **Template Filling**, find the correct entity to fill a certain template, for instance.\n",
    "\n",
    "In this BLU we are going to learn some of the basic techniques to extract specific (pre-specified) information from textual sources. From the three specified tasks, we are going to **focus on the task of named-entity recognition (NER)** where our objective is to **retrieve all the mentions** of entities like people, locations, time, among others. The other two are mentioned for the sake of completeness and you should definitely research more about them, specially if you're eager to learn more about NLP.\n",
    "\n",
    "![robot entities](./media/robot_entities.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on a corpus containing forum discussions. We extracted a sample from Reddit for this use. For more interesting examples, you may find more textual data available at https://files.pushshift.io/reddit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.6 MB 3.3 MB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 13.3 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in ./venv/lib/python3.8/site-packages (from en-core-web-sm==3.1.0) (3.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.21.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in ./venv/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./venv/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in ./venv/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.10.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.2)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in ./venv/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read 1000 documents\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('./datasets/sample_data.json') as fp:\n",
    "    for line in fp:\n",
    "        entry = json.loads(line)\n",
    "        docs.append(entry['body'])\n",
    "        \n",
    "print('I read {} documents'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction with Regular Expressions\n",
    "\n",
    "In BLU07, we became pros of regular expressions. In this BLU, we're going to try to use them to our task of recognizing entities. Take a moment to think about all the possibilities of Entities that we can find in a text. Do you think such a task will be achievable using only regular expressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regex](./media/regex.gif \"regex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, let's say that your boss asked you to retrieve all the **dates** mentioned in our sample corpus. We learned in BLU7 that, if we follow a certain pattern for the dates, it is easy to use a regular expression for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14/09/30', '7/12/2007', '4/16/2007', '3/27/2007', '2/28/2007']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find all possible dates in the format xx/xx/xxxx\n",
    "data = ' '.join(docs)\n",
    "re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks like it's going to be a breeze. However, now your boss decides to ask you to retrieve all the **country names** which appear in the corpus instead. \n",
    "\n",
    "One possible approach is to retrieve a list of all countries that exist and look for the occurence of such elements in the corpus. Let's try that, shall we?\n",
    "\n",
    "![alt text](./media/countries_meme.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "with open('./datasets/countries.txt') as fp:\n",
    "    for line in fp:\n",
    "        countries.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use again regular expressions for this. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('us', 763, 765)\n",
      "('United States', 827, 840)\n",
      "('UK', 6971, 6973)\n",
      "('US', 7000, 7002)\n",
      "('Puerto rico', 8026, 8037)\n",
      "('us', 8638, 8640)\n",
      "('France', 19815, 19821)\n",
      "('us', 21563, 21565)\n",
      "('Puerto Rico', 27659, 27670)\n",
      "('Puerto Rico', 27754, 27765)\n",
      "('US', 28101, 28103)\n",
      "('Canada', 29439, 29445)\n",
      "('USA', 32880, 32883)\n",
      "('Norway', 34749, 34755)\n",
      "('Korea', 34837, 34842)\n",
      "('USA', 35738, 35741)\n",
      "('United States', 41060, 41073)\n",
      "('us', 42290, 42292)\n",
      "('us', 42403, 42405)\n",
      "('Soviet', 44563, 44569)\n",
      "('us', 49625, 49627)\n",
      "('Chad', 51352, 51356)\n"
     ]
    }
   ],
   "source": [
    "# Sort country list by length. This is important to match longer spans before short \n",
    "# ones (like in 'Papua New Guinea' vs. 'Papua')\n",
    "countries.sort(key=len, reverse=True)\n",
    "\n",
    "# Make a regex to recognize all possible names.\n",
    "# '|' creates the or operation in regex\n",
    "# \\b means word boundaries (punctuation or white spaces)\n",
    "# re.escape is used to escape regex operators like '.'    \n",
    "countries_regex = r'\\b(' + '|'.join([re.escape(c) for c in countries]) + r')\\b'\n",
    "\n",
    "# finditer is similar to findall\n",
    "# the flag re.I means to ignore casing (accept both lowercase and uppercase letters as the same)\n",
    "for i, m in enumerate(re.finditer(countries_regex, data, flags=re.I)):\n",
    "    print( (m.group(), m.start(), m.end()) )\n",
    "    # just show the first 20\n",
    "    if i > 20:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this approach working?**\n",
    "\n",
    "It seems like the word **'us'**, for example, has caused some confusion. It could be the country _U.S._, or just the pronoun _us_. In this case, just comparing the word form we are not able to disambiguate the two forms. We will need either more **context** or more **linguistic information** and regular expressions won't give us none of that.\n",
    "\n",
    "Luckily, you already know an NLP library which can provide you the correct information to disambiguate the word 'us'. In the next examples, we will use SpaCy as our NLP toolkit to give us just that.\n",
    "\n",
    "## Deeper look into information extraction using SpaCy\n",
    "![Spacy](./media/spacy.jpg)\n",
    "\n",
    "If you remember BLU08, we used SpaCy to understand word vectors (aka word embeddings). We will make use of the medium sized SpaCy english model once again. In case you haven't downloaded it yet, you can simply use this command in a Notebook cell (this is the same cell command of the one at the top of this notebook):\n",
    "\n",
    "```\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n",
    "    \n",
    "But of course we could have used any english model (en_core_web_sm, en_core_web_md, en_core_web_lg) provided by SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are disabling the synctatic parser from the pipeline to improve the loading speed.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SpaCy, we will process the documents with the complete NLP pipeline using [pipe](https://spacy.io/usage/processing-pipelines). This means that `pipe` will process our text, tokenize it and extract information from it using all the CPU cores from our machine. Concretely, it will Part-of-Speech tag (more on that later), parse and extract entities.\n",
    "\n",
    "We won't get into details on how SpaCy does this -- what matters is that it uses pretty regular machine learning models, achieving good enough accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use the function pipe to process all documents.\n",
    "# One of the strenghts for SpaCy is the parallel processing using all your computer cores.\n",
    "# In this step, SpaCy performs the NLP pipeline for all the docs, so it may take a while\n",
    "\n",
    "docs = list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to do NER (Named Entity Recognition) in a piece of text. We can get an example sentence from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was more like $10 million dollars after the previous government infringed on his rights within the Charter of Rights and Freedoms (basically the Canadian constitution). Trudeau knew Khadr would win in court, and settled for paying $10 million instead of an amount multiple times more. \n",
      "\n",
      "Not saying I'm happy with Khadr getting $10 million, but this was more of a fuck up on the previous government since they blatantly violated his rights as a Canadian.\n"
     ]
    }
   ],
   "source": [
    "example = docs[250]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SpaCy, it's really easy to extract entities - we can simply use `.ents` in our previously processed text, and SpaCy will use its built-in model to get the entities present in the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$10 million dollars 17 36 MONEY\n",
      "the Charter of Rights and Freedoms 98 132 WORK_OF_ART\n",
      "Canadian 148 156 NORP\n",
      "Trudeau 172 179 PERSON\n",
      "Khadr 185 190 PERSON\n",
      "$10 million 234 245 MONEY\n",
      "Khadr 316 321 PERSON\n",
      "$10 million 330 341 MONEY\n",
      "Canadian 447 455 NORP\n"
     ]
    }
   ],
   "source": [
    "for ent in example.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example sentence, SpaCy gave correct labels for our examples, such as Trudeau and Khadr are PERSON, and Canadian is NORP (Nationalities or religious or political groups). You can find here the complete [list of different entity types](https://spacy.io/usage/linguistic-features#accessing-ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is processed and we know how to get entities, let's build a `Matcher` in SpaCy.\n",
    "\n",
    "A `Matcher` is SpaCy's version of a regular expression - it searches for patterns in your text, according to the rules you give it. However, it is much more powerful since it has access to the outputs of the aforementioned NLP pipeline. That means we can search patterns that include certain entities or Part-of-Speech tags. \n",
    "\n",
    "In this `Matcher` we will define templates which we will use later to match elements in the text (thus using it to do information extraction). The `Matcher` is initialized using the vocabulary object, which must be shared with the documents the matcher will operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # Pass the vocabulary object to Matcher.__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a similar matcher as we did above with regular expressions. We are going to get each country name and add it as a pattern to the `matcher`. To add a pattern, we can simply use `.add()`. It receives:\n",
    "\n",
    "- an ID (the name we want to give our pattern)\n",
    "- the pattern itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # Build a pattern from the country name. For example: United States -> [{'LOWER': 'united'}, {'LOWER': 'states'}]\n",
    "    # LOWER means to match the words in the lowercased token.\n",
    "    pattern = [[{'LOWER': c.lower()} for c in country.split()]]\n",
    "    matcher.add(country, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 2 us\n",
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "69 50 51 us\n",
      "146 4 5 France\n",
      "167 29 30 us\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "320 35 36 us\n",
      "320 58 59 us\n",
      "335 38 39 Soviet\n",
      "335 38 39 Soviet\n",
      "349 4 5 us\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "# for screen economy, let's just show the matches for the first 400 documents.\n",
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previously mentioned, in order to disambiguate the retrieval of 'U.S.' vs 'us' we need to add more linguistic information to the `matcher`. Let's play with Part of Speech (PoS).\n",
    "\n",
    "## But what is Part-of-Speech?\n",
    "\n",
    "If you remember from your language classes, you could categorize words in a sentence according to the role they have in it. In NLP, we call this Part of Speech tags. For the English language, common PoS tags are: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection.\n",
    "\n",
    "SpaCy adopts the Universal PoS tagset where any language has a common subset of PoS defined. The list of all possible values can be consulted [here](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py).\n",
    "\n",
    "In this case, we are interested in matching the country names that were tagged as **Proper Nouns** ('PROPN' tag obtained from the tagset list). A Proper Noun is a specific (i.e., not generic) name for a particular person, place, or thing.\n",
    "\n",
    "![Pronoun meme](./media/pronoun.jpg)\n",
    "\n",
    "In SpaCy, just as entities of a document are inside `doc.ents`, for each token of a document we can find its assigned POS tag by using `.pos_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It PRON\n",
      "was VERB\n",
      "more ADV\n",
      "like ADP\n",
      "$ SYM\n",
      "10 NUM\n",
      "million NUM\n",
      "dollars NOUN\n",
      "after ADP\n",
      "the DET\n",
      "previous ADJ\n",
      "government NOUN\n",
      "infringed VERB\n",
      "on ADP\n",
      "his PRON\n",
      "rights NOUN\n",
      "within ADP\n",
      "the DET\n",
      "Charter PROPN\n",
      "of ADP\n",
      "Rights PROPN\n",
      "and CCONJ\n",
      "Freedoms PROPN\n",
      "( PUNCT\n",
      "basically ADV\n",
      "the DET\n",
      "Canadian ADJ\n",
      "constitution NOUN\n",
      ") PUNCT\n",
      ". PUNCT\n",
      "Trudeau PROPN\n",
      "knew VERB\n",
      "Khadr PROPN\n",
      "would AUX\n",
      "win VERB\n",
      "in ADP\n",
      "court NOUN\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "settled VERB\n",
      "for ADP\n",
      "paying VERB\n",
      "$ SYM\n",
      "10 NUM\n",
      "million NUM\n",
      "instead ADV\n",
      "of ADP\n",
      "an DET\n",
      "amount NOUN\n",
      "multiple ADJ\n",
      "times NOUN\n",
      "more ADJ\n",
      ". PUNCT\n",
      "\n",
      "\n",
      " SPACE\n",
      "Not PART\n",
      "saying VERB\n",
      "I PRON\n",
      "'m VERB\n",
      "happy ADJ\n",
      "with ADP\n",
      "Khadr PROPN\n",
      "getting VERB\n",
      "$ SYM\n",
      "10 NUM\n",
      "million NUM\n",
      ", PUNCT\n",
      "but CCONJ\n",
      "this DET\n",
      "was VERB\n",
      "more ADJ\n",
      "of ADP\n",
      "a DET\n",
      "fuck NOUN\n",
      "up ADP\n",
      "on ADP\n",
      "the DET\n",
      "previous ADJ\n",
      "government NOUN\n",
      "since SCONJ\n",
      "they PRON\n",
      "blatantly ADV\n",
      "violated VERB\n",
      "his PRON\n",
      "rights NOUN\n",
      "as ADP\n",
      "a DET\n",
      "Canadian PROPN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in example:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `Matcher` is pretty smart, so we only really need to add to a `'POS'` entry in the pattern dictionary and the tag we are looking for as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new matcher instance\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for country in countries:\n",
    "    # same as before, but now with one more restriction: the Part-of-speech should be a Proper Noun.\n",
    "    pattern = [[{'LOWER': c.lower(), 'POS': 'PROPN'} for c in country.split()]]\n",
    "    matcher.add(country, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "146 4 5 France\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start:end]\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the PoS tagger is based on a machine learning method, so it is prone to errors. Notice how it causes _Puerto rico_ of document 64 to be out of this list. Moreover, _Chad_ could also be interpreted as a Name or a Country (allusive to Republic of Chad). State of the art is always improving, nowadays, there are techniques that take context into account as well to improve pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting using complex patterns\n",
    "\n",
    "Let's now look into other types of information extraction methods which use complex structures. For example, let's say we want to extract places. Usually, places come up in text in structures similar to:\n",
    "\n",
    "* go to xx\n",
    "* went from xxx\n",
    "* going to xx\n",
    "\n",
    "**Note**: Notice that such patterns could be interesting to the task of relation extraction we mentioned in the intro. But that's something we will leave up to you to look further into.\n",
    "\n",
    "In order to build a SpaCy pattern for the proposed sentence structure, we are going to use the lemma word 'go' (remember lemmatization from BLU07? We can do this in SpaCy pretty easily as well!), which is invariant for all possible verb inflections, a preposition (POS tag name - ADP) and a proper noun (POS tag name - PROPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [[{'LEMMA': 'go'}, {'POS': 'ADP'}, {'POS': 'PROPN'}]]\n",
    "matcher.add('LOC', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 5 go for Sorcery\n",
      "24 27 goes to GTA\n",
      "246 249 going to Osaka\n",
      "81 84 gone to Irvine\n",
      "91 94 going with Robbie\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        print(start, end, span_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sure aren't all the locations that are present in our corpus! Not what we expected then üôÉ \n",
    "\n",
    "Once again, we are finding out that it is very difficult to build patterns to match these type of ocurrences in the text. Addressing all possible patterns for person, location, etc. this way is very inneficient and difficult. \n",
    "\n",
    "Another possible way to go is to annotate examples in a corpus. We can train machine learning systems from scratch to automatically extract patterns from annotated corpora. Such class of machine learning methods are known as sequencial labeling and the most famous approaches are [CRFs](https://people.cs.umass.edu/~wallach/technical_reports/wallach04conditional.pdf) and [Seq2seq](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) and now most recently [Transformers](https://jalammar.github.io/illustrated-bert/). You are not expected to know what these models do since they are too complex right now for you to understand, but it's good to keep them in mind and maybe play a little bit if you wish üòä\n",
    "\n",
    "Fortunately, as explained above, Spacy already contains pre-trained models for standard named-entities. There are several which you can find on the list shared previously. Besides _Person_ (PER) entities like _Bilbo_ and _Organization_ (ORG) entities like _PayPal_ , we can also extract _Location_ entities with the code GPE!\n",
    "\n",
    "Let's try to extract all Locations using the built-in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 New York 0 8\n",
      "12 United States Anonymous 16 39\n",
      "30 Portland 5 13\n",
      "30 Portland 402 410\n",
      "49 Hoover 26 32\n",
      "58 UK 117 119\n",
      "58 US 146 148\n",
      "64 Puerto rico 81 92\n",
      "76 Muana 44 49\n",
      "87 Islam 199 204\n",
      "129 Cordelia 30 38\n",
      "129 IV 186 188\n",
      "156 VT 3 5\n",
      "202 Tennessee 107 116\n",
      "213 Puerto Ricos 195 207\n",
      "213 Puerto Rico 467 478\n",
      "213 Puerto Rico 562 573\n",
      "213 US 909 911\n",
      "226 North Moorhead 2-3 years ago 16 44\n",
      "229 Canada 16 22\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:250]):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'GPE':   \n",
    "            print(i, e.text, e.start_char, e.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, let's not forget that, as in any machine learning model, we are also prone to errors in our prediction.\n",
    "\n",
    "Could we train a better model? Sure! Given a good corpus for training and the right tools we could achieve a very high accuracy. However, as this is not the objective of this BLU we are going to leave you some links if you want to learn more about this.\n",
    "\n",
    "https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another handy link - https://spacy.io/usage/linguistic-features - you can find here all kind of features SpaCy can extract for you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
